
@misc{ilharco_editing_2023,
	title = {Editing Models with Task Arithmetic},
	url = {http://arxiv.org/abs/2212.04089},
	doi = {10.48550/arXiv.2212.04089},
	abstract = {Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around {\textbackslash}textit\{task vectors\}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models.},
	number = {{arXiv}:2212.04089},
	publisher = {{arXiv}},
	author = {Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
	urldate = {2024-04-21},
	date = {2023-03-31},
	eprinttype = {arxiv},
	eprint = {2212.04089 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:files/1048/Ilharco et al. - 2023 - Editing Models with Task Arithmetic.pdf:application/pdf;arXiv.org Snapshot:files/1049/2212.html:text/html},
}

@misc{chronopoulou_language_2023,
	title = {Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization},
	url = {http://arxiv.org/abs/2311.09344},
	doi = {10.48550/arXiv.2311.09344},
	abstract = {Parameter-efficient fine-tuning ({PEFT}) using labeled task data can significantly improve the performance of large language models ({LLMs}) on the downstream task. However, there are 7000 languages in the world and many of these languages lack labeled data for real-world language generation tasks. In this paper, we propose to improve zero-shot cross-lingual transfer by composing language or task specialized parameters. Our method composes language and task {PEFT} modules via element-wise arithmetic operations to leverage unlabeled data and English labeled data. We extend our approach to cases where labeled data from more languages is available and propose to arithmetically compose {PEFT} modules trained on languages related to the target. Empirical results on summarization demonstrate that our method is an effective strategy that obtains consistent gains using minimal training of {PEFT} modules.},
	number = {{arXiv}:2311.09344},
	publisher = {{arXiv}},
	author = {Chronopoulou, Alexandra and Pfeiffer, Jonas and Maynez, Joshua and Wang, Xinyi and Ruder, Sebastian and Agrawal, Priyanka},
	urldate = {2024-04-21},
	date = {2023-11-15},
	eprinttype = {arxiv},
	eprint = {2311.09344 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/1051/Chronopoulou et al. - 2023 - Language and Task Arithmetic with Parameter-Effici.pdf:application/pdf;arXiv.org Snapshot:files/1052/2311.html:text/html},
}

@misc{pfeiffer_modular_2024,
	title = {Modular Deep Learning},
	url = {http://arxiv.org/abs/2302.11529},
	doi = {10.48550/arXiv.2302.11529},
	abstract = {Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.},
	number = {{arXiv}:2302.11529},
	publisher = {{arXiv}},
	author = {Pfeiffer, Jonas and Ruder, Sebastian and Vulić, Ivan and Ponti, Edoardo Maria},
	urldate = {2024-04-21},
	date = {2024-01-27},
	eprinttype = {arxiv},
	eprint = {2302.11529 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/1054/Pfeiffer et al. - 2024 - Modular Deep Learning.pdf:application/pdf;arXiv.org Snapshot:files/1055/2302.html:text/html},
}

@misc{hu_llm-adapters_2023,
	title = {{LLM}-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
	url = {http://arxiv.org/abs/2304.01933},
	doi = {10.48550/arXiv.2304.01933},
	shorttitle = {{LLM}-Adapters},
	abstract = {The success of large language models ({LLMs}), like {GPT}-4 and {ChatGPT}, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access {LLMs} with task-specific data (e.g., {ChatDoctor}) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning ({PEFT}) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire {LLMs} while achieving comparable or even better performance. To enable further research on {PEFT} methods of {LLMs}, this paper presents {LLM}-Adapters, an easy-to-use framework that integrates various adapters into {LLMs} and can execute these adapter-based {PEFT} methods of {LLMs} for different tasks. The framework includes state-of-the-art open-access {LLMs} such as {LLaMA}, {BLOOM}, and {GPT}-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based {PEFT} in smaller-scale {LLMs} (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful {LLMs} (175B) in zero-shot inference on both reasoning tasks.},
	number = {{arXiv}:2304.01933},
	publisher = {{arXiv}},
	author = {Hu, Zhiqiang and Wang, Lei and Lan, Yihuai and Xu, Wanyu and Lim, Ee-Peng and Bing, Lidong and Xu, Xing and Poria, Soujanya and Lee, Roy Ka-Wei},
	urldate = {2024-04-22},
	date = {2023-10-09},
	eprinttype = {arxiv},
	eprint = {2304.01933 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/1058/Hu et al. - 2023 - LLM-Adapters An Adapter Family for Parameter-Effi.pdf:application/pdf;arXiv.org Snapshot:files/1059/2304.html:text/html},
}

@online{lialin_scaling_2023,
	title = {Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning},
	url = {https://arxiv.org/abs/2303.15647v1},
	shorttitle = {Scaling Down to Scale Up},
	abstract = {This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.},
	titleaddon = {{arXiv}.org},
	author = {Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
	urldate = {2024-04-22},
	date = {2023-03-28},
	langid = {english},
	file = {Full Text PDF:files/1061/Lialin et al. - 2023 - Scaling Down to Scale Up A Guide to Parameter-Eff.pdf:application/pdf},
}

@misc{houlsby_parameter-efficient_2019,
	title = {Parameter-Efficient Transfer Learning for {NLP}},
	url = {http://arxiv.org/abs/1902.00751},
	doi = {10.48550/arXiv.1902.00751},
	abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in {NLP}. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed {BERT} Transformer model to 26 diverse text classification tasks, including the {GLUE} benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On {GLUE}, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
	number = {{arXiv}:1902.00751},
	publisher = {{arXiv}},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	urldate = {2024-04-22},
	date = {2019-06-13},
	eprinttype = {arxiv},
	eprint = {1902.00751 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:files/1063/Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf:application/pdf;arXiv.org Snapshot:files/1064/1902.html:text/html},
}

@misc{xu_parameter-efficient_2023,
	title = {Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment},
	url = {http://arxiv.org/abs/2312.12148},
	doi = {10.48550/arXiv.2312.12148},
	shorttitle = {Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models},
	abstract = {With the continuous growth in the number of parameters of transformer-based pretrained language models ({PLMs}), particularly the emergence of large language models ({LLMs}) with billions of parameters, many natural language processing ({NLP}) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter Efficient Fine-Tuning ({PEFT}) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning {PLMs}, especially {LLMs}, have led to a surge in the development of {PEFT} methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of {PEFT} methods for {PLMs}. We summarize these {PEFT} methods, discuss their applications, and outline future directions. Furthermore, we conduct experiments using several representative {PEFT} methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by {PEFT} in the context of {PLMs}.},
	number = {{arXiv}:2312.12148},
	publisher = {{arXiv}},
	author = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
	urldate = {2024-04-22},
	date = {2023-12-19},
	eprinttype = {arxiv},
	eprint = {2312.12148 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/1067/Xu et al. - 2023 - Parameter-Efficient Fine-Tuning Methods for Pretra.pdf:application/pdf;arXiv.org Snapshot:files/1068/2312.html:text/html},
}

@misc{han_parameter-efficient_2024,
	title = {Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey},
	url = {http://arxiv.org/abs/2403.14608},
	doi = {10.48550/arXiv.2403.14608},
	shorttitle = {Parameter-Efficient Fine-Tuning for Large Models},
	abstract = {Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning ({PEFT}) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, {PEFT} refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various {PEFT} algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different {PEFT} algorithms and discuss common techniques employed to mitigate computation costs for {PEFT}. In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different {PEFT} algorithms. This survey serves as an indispensable resource for researchers aiming to understand both the {PEFT} algorithm and its system implementation, offering detailed insights into recent advancements and practical applications.},
	number = {{arXiv}:2403.14608},
	publisher = {{arXiv}},
	author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
	urldate = {2024-04-22},
	date = {2024-04-17},
	eprinttype = {arxiv},
	eprint = {2403.14608 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/1071/Han et al. - 2024 - Parameter-Efficient Fine-Tuning for Large Models .pdf:application/pdf;arXiv.org Snapshot:files/1072/2403.html:text/html},
}

@misc{su_unlocking_2024,
	title = {Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation},
	url = {http://arxiv.org/abs/2404.04212},
	doi = {10.48550/arXiv.2404.04212},
	abstract = {Parameter-efficient fine-tuning ({PEFT}) methods are increasingly vital in adapting large-scale pre-trained language models for diverse tasks, offering a balance between adaptability and computational efficiency. They are important in Low-Resource Language ({LRL}) Neural Machine Translation ({NMT}) to enhance translation accuracy with minimal resources. However, their practical effectiveness varies significantly across different languages. We conducted comprehensive empirical experiments with varying {LRL} domains and sizes to evaluate the performance of 8 {PEFT} methods with in total of 15 architectures using the {SacreBLEU} score. We showed that 6 {PEFT} architectures outperform the baseline for both in-domain and out-domain tests and the Houlsby+Inversion adapter has the best performance overall, proving the effectiveness of {PEFT} methods.},
	number = {{arXiv}:2404.04212},
	publisher = {{arXiv}},
	author = {Su, Tong and Peng, Xin and Thillainathan, Sarubi and Guzmán, David and Ranathunga, Surangika and Lee, En-Shiun Annie},
	urldate = {2024-04-22},
	date = {2024-04-05},
	eprinttype = {arxiv},
	eprint = {2404.04212 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/1075/Su et al. - 2024 - Unlocking Parameter-Efficient Fine-Tuning for Low-.pdf:application/pdf;arXiv.org Snapshot:files/1076/2404.html:text/html},
}

@inproceedings{ruder_modular_2022,
	location = {Abu Dubai, {UAE}},
	title = {Modular and Parameter-Efficient Fine-Tuning for {NLP} Models},
	url = {https://aclanthology.org/2022.emnlp-tutorials.5},
	doi = {10.18653/v1/2022.emnlp-tutorials.5},
	abstract = {State-of-the-art language models in {NLP} perform best when fine-tuned even on small datasets, but due to their increasing size, fine-tuning and downstream usage have become extremely compute-intensive. Being able to efficiently and effectively fine-tune the largest pre-trained models is thus key in order to reap the benefits of the latest advances in {NLP}. In this tutorial, we provide a comprehensive overview of parameter-efficient fine-tuning methods. We highlight their similarities and differences by presenting them in a unified view. We explore the benefits and usage scenarios of a neglected property of such parameter-efficient models—modularity—such as composition of modules to deal with previously unseen data conditions. We finally highlight how both properties——parameter efficiency and modularity——can be useful in the real-world setting of adapting pre-trained models to under-represented languages and domains with scarce annotated data for several downstream applications.},
	pages = {23--29},
	booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts},
	publisher = {Association for Computational Linguistics},
	author = {Ruder, Sebastian and Pfeiffer, Jonas and Vulić, Ivan},
	editor = {El-Beltagy, Samhaa R. and Qiu, Xipeng},
	urldate = {2024-04-22},
	date = {2022-12},
	file = {Full Text PDF:files/1078/Ruder et al. - 2022 - Modular and Parameter-Efficient Fine-Tuning for NL.pdf:application/pdf},
}

@misc{poth_adapters_2023,
	title = {Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning},
	url = {http://arxiv.org/abs/2311.11077},
	doi = {10.48550/arXiv.2311.11077},
	shorttitle = {Adapters},
	abstract = {We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library's efficacy by evaluating its performance against full fine-tuning on various {NLP} tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters.},
	number = {{arXiv}:2311.11077},
	publisher = {{arXiv}},
	author = {Poth, Clifton and Sterz, Hannah and Paul, Indraneil and Purkayastha, Sukannya and Engländer, Leon and Imhof, Timo and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna and Pfeiffer, Jonas},
	urldate = {2024-04-22},
	date = {2023-11-18},
	eprinttype = {arxiv},
	eprint = {2311.11077 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/1081/Poth et al. - 2023 - Adapters A Unified Library for Parameter-Efficien.pdf:application/pdf;arXiv.org Snapshot:files/1082/2311.html:text/html},
}

@misc{gjurkovic_pandora_2021,
	title = {{PANDORA} Talks: Personality and Demographics on Reddit},
	url = {http://arxiv.org/abs/2004.04460},
	doi = {10.48550/arXiv.2004.04460},
	shorttitle = {{PANDORA} Talks},
	abstract = {Personality and demographics are important variables in social sciences, while in {NLP} they can aid in interpretability and removal of societal biases. However, datasets with both personality and demographic labels are scarce. To address this, we present {PANDORA}, the first large-scale dataset of Reddit comments labeled with three personality models (including the well-established Big 5 model) and demographics (age, gender, and location) for more than 10k users. We showcase the usefulness of this dataset on three experiments, where we leverage the more readily available data from other personality models to predict the Big 5 traits, analyze gender classification biases arising from psycho-demographic variables, and carry out a confirmatory and exploratory analysis based on psychological theories. Finally, we present benchmark prediction models for all personality and demographic variables.},
	number = {{arXiv}:2004.04460},
	publisher = {{arXiv}},
	author = {Gjurković, Matej and Karan, Mladen and Vukojević, Iva and Bošnjak, Mihaela and Šnajder, Jan},
	urldate = {2024-09-11},
	date = {2021-06-08},
	eprinttype = {arxiv},
	eprint = {2004.04460 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:files/1086/Gjurković et al. - 2021 - PANDORA Talks Personality and Demographics on Red.pdf:application/pdf;arXiv.org Snapshot:files/1087/2004.html:text/html},
}

@misc{song_identifying_2024,
	title = {Identifying Multiple Personalities in Large Language Models with External Evaluation},
	url = {http://arxiv.org/abs/2402.14805},
	doi = {10.48550/arXiv.2402.14805},
	abstract = {As Large Language Models ({LLMs}) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of {LLMs}. One of the ways to comprehend {LLMs}' behavior is to analyze their personalities. Many recent studies quantify {LLMs}' personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to {LLMs}. In this paper, we investigate {LLM} personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting {LLMs} with multiple-choice questions in the Likert scale, we evaluate {LLMs}' personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the {MBTI} personality predictor that outperforms the state-of-the-art models as the tool to analyze {LLMs}' responses. Then, we prompt the {LLMs} with situational questions and ask them to generate Twitter posts and comments, respectively, in order to assess their personalities when playing two different roles. Using the external personality evaluation method, we identify that the obtained personality types for {LLMs} are significantly different when generating posts versus comments, whereas humans show a consistent personality profile in these two different situations. This shows that {LLMs} can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in {LLMs} and humans. With our work, we call for a re-evaluation of personality definition and measurement in {LLMs}.},
	number = {{arXiv}:2402.14805},
	publisher = {{arXiv}},
	author = {Song, Xiaoyang and Adachi, Yuta and Feng, Jessie and Lin, Mouwei and Yu, Linhao and Li, Frank and Gupta, Akshat and Anumanchipalli, Gopala and Kaur, Simerjot},
	urldate = {2024-09-19},
	date = {2024-02-22},
	eprinttype = {arxiv},
	eprint = {2402.14805 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:files/1089/Song et al. - 2024 - Identifying Multiple Personalities in Large Langua.pdf:application/pdf;arXiv.org Snapshot:files/1090/2402.html:text/html},
}

@misc{jiang_evaluating_2023,
	title = {Evaluating and Inducing Personality in Pre-trained Language Models},
	url = {http://arxiv.org/abs/2206.07550},
	doi = {10.48550/arXiv.2206.07550},
	abstract = {Standardized and quantified evaluation of machine behaviors is a crux of understanding {LLMs}. In this study, we draw inspiration from psychometric studies by leveraging human personality theory as a tool for studying machine behaviors. Originating as a philosophical quest for human behaviors, the study of personality delves into how individuals differ in thinking, feeling, and behaving. Toward building and understanding human-like social machines, we are motivated to ask: Can we assess machine behaviors by leveraging human psychometric tests in a principled and quantitative manner? If so, can we induce a specific personality in {LLMs}? To answer these questions, we introduce the Machine Personality Inventory ({MPI}) tool for studying machine behaviors; {MPI} follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating {LLMs} with {MPI}, we provide the first piece of evidence demonstrating the efficacy of {MPI} in studying {LLMs} behaviors. We further devise a Personality Prompting (P{\textasciicircum}2) method to induce {LLMs} with specific personalities in a controllable way, capable of producing diverse and verifiable behaviors. We hope this work sheds light on future studies by adopting personality as the essential indicator for various downstream tasks, and could further motivate research into equally intriguing human-like machine behaviors.},
	number = {{arXiv}:2206.07550},
	publisher = {{arXiv}},
	author = {Jiang, Guangyuan and Xu, Manjie and Zhu, Song-Chun and Han, Wenjuan and Zhang, Chi and Zhu, Yixin},
	urldate = {2024-09-19},
	date = {2023-10-29},
	eprinttype = {arxiv},
	eprint = {2206.07550 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/1093/Jiang et al. - 2023 - Evaluating and Inducing Personality in Pre-trained.pdf:application/pdf;arXiv.org Snapshot:files/1094/2206.html:text/html},
}

@book{safdari_personality_2023,
	title = {Personality Traits in Large Language Models},
	abstract = {The advent of large language models ({LLMs}) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As {LLMs} increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used {LLMs}. We find that: 1) personality simulated in the outputs of some {LLMs} (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of {LLM}-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in {LLM} outputs can be shaped along desired dimensions to mimic specific personality profiles. We also discuss potential applications and ethical implications of our measurement and shaping framework, especially regarding responsible use of {LLMs}.},
	author = {Safdari, Mustafa and Serapio-García, Greg and Crepy, Clément and Fitz, Stephen and Romero, Peter and Sun, Luning and Abdulhai, Marwa and Faust, Aleksandra and Matarić, Maja},
	date = {2023-06-30},
	doi = {10.21203/rs.3.rs-3296728/v1},
	file = {Submitted Version:files/1097/Safdari et al. - 2023 - Personality Traits in Large Language Models.pdf:application/pdf},
}

@online{allbert_identifying_nodate,
	title = {Identifying and Manipulating Personality Traits in Large Language Models ({LLMs}) - Online Technical Discussion Groups—Wolfram Community},
	url = {https://community.wolfram.com/groups/-/m/t/3210605?source=frontpage-news},
	abstract = {Wolfram Community forum discussion about [{WSS}24] Identifying and Manipulating Personality Traits in Large Language Models ({LLMs}). Stay on top of important topics and build connections by joining Wolfram Community groups relevant to your interests.},
	author = {Allbert, Rumi},
	urldate = {2024-10-07},
	langid = {american},
	file = {Snapshot:files/1099/3210605.html:text/html},
}

@inproceedings{sorokovikova_llms_2024,
	location = {St. Julians, Malta},
	title = {{LLMs} Simulate Big5 Personality Traits: Further Evidence},
	url = {https://aclanthology.org/2024.personalize-1.7},
	shorttitle = {{LLMs} Simulate Big5 Personality Traits},
	abstract = {An empirical investigation into the simulation of the Big5 personality traits by large language models ({LLMs}), namely Llama-2, {GPT}-4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of {LLMs} to simulate personality traits and the respective implications for personalized human-computer interaction.},
	pages = {83--87},
	booktitle = {Proceedings of the 1st Workshop on Personalization of Generative {AI} Systems ({PERSONALIZE} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Sorokovikova, Aleksandra and Rezagholi, Sharwin and Fedorova, Natalia and Yamshchikov, Ivan P.},
	editor = {Deshpande, Ameet and Hwang, {EunJeong} and Murahari, Vishvak and Park, Joon Sung and Yang, Diyi and Sabharwal, Ashish and Narasimhan, Karthik and Kalyan, Ashwin},
	urldate = {2024-10-07},
	date = {2024-03},
	file = {Full Text PDF:files/1101/Sorokovikova et al. - 2024 - LLMs Simulate Big5 Personality Traits Further Evidence.pdf:application/pdf},
}

@inproceedings{frisch_llm_2024,
	location = {St. Julians, Malta},
	title = {{LLM} Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models},
	url = {https://aclanthology.org/2024.personalize-1.9},
	shorttitle = {{LLM} Agents in Interaction},
	abstract = {Agent interaction has long been a key topic in psychology, philosophy, and artificial intelligence, and it is now gaining traction in large language model ({LLM}) research. This experimental study seeks to lay the groundwork for our understanding of dialogue-based interaction between {LLMs}: Do persona-prompted {LLMs} show consistent personality and language use in interaction? We condition {GPT}-3.5 on asymmetric personality profiles to create a population of {LLM} agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.},
	pages = {102--111},
	booktitle = {Proceedings of the 1st Workshop on Personalization of Generative {AI} Systems ({PERSONALIZE} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Frisch, Ivar and Giulianelli, Mario},
	editor = {Deshpande, Ameet and Hwang, {EunJeong} and Murahari, Vishvak and Park, Joon Sung and Yang, Diyi and Sabharwal, Ashish and Narasimhan, Karthik and Kalyan, Ashwin},
	urldate = {2024-10-07},
	date = {2024-03},
	file = {Full Text PDF:files/1103/Frisch and Giulianelli - 2024 - LLM Agents in Interaction Measuring Personality Consistency and Linguistic Alignment in Interacting.pdf:application/pdf},
}

@misc{lee_llms_2024,
	title = {Do {LLMs} Have Distinct and Consistent Personality? {TRAIT}: Personality Testset designed for {LLMs} with Psychometrics},
	url = {http://arxiv.org/abs/2406.14703},
	shorttitle = {Do {LLMs} Have Distinct and Consistent Personality?},
	abstract = {The idea of personality in descriptive psychology, traditionally defined through observable behavior, has now been extended to Large Language Models ({LLMs}) to better understand their behavior. This raises a question: do {LLMs} exhibit distinct and consistent personality traits, similar to humans? Existing self-assessment personality tests, while applicable, lack the necessary validity and reliability for precise personality measurements. To address this, we introduce {TRAIT}, a new tool consisting of 8K multi-choice questions designed to assess the personality of {LLMs} with validity and reliability. {TRAIT} is built on the psychometrically validated human questionnaire, Big Five Inventory ({BFI}) and Short Dark Triad ({SD}-3), enhanced with the {ATOMIC}10X knowledge graph for testing personality in a variety of real scenarios. {TRAIT} overcomes the reliability and validity issues when measuring personality of {LLM} with self-assessment, showing the highest scores across three metrics: refusal rate, prompt sensitivity, and option order sensitivity. It reveals notable insights into personality of {LLM}: 1) {LLMs} exhibit distinct and consistent personality, which is highly influenced by their training data (i.e., data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction.},
	number = {{arXiv}:2406.14703},
	publisher = {{arXiv}},
	author = {Lee, Seungbeen and Lim, Seungwon and Han, Seungju and Oh, Giyeong and Chae, Hyungjoo and Chung, Jiwan and Kim, Minju and Kwak, Beong-woo and Lee, Yeonsoo and Lee, Dongha and Yeo, Jinyoung and Yu, Youngjae},
	urldate = {2024-10-09},
	date = {2024-06-20},
	eprinttype = {arxiv},
	eprint = {2406.14703},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1107/Lee et al. - 2024 - Do LLMs Have Distinct and Consistent Personality TRAIT Personality Testset designed for LLMs with.pdf:application/pdf;Snapshot:files/1108/2406.html:text/html},
}

@misc{zhang_better_2024,
	title = {The Better Angels of Machine Personality: How Personality Relates to {LLM} Safety},
	url = {http://arxiv.org/abs/2407.12344},
	shorttitle = {The Better Angels of Machine Personality},
	abstract = {Personality psychologists have analyzed the relationship between personality and safety behaviors in human society. Although Large Language Models ({LLMs}) demonstrate personality traits, the relationship between personality traits and safety abilities in {LLMs} still remains a mystery. In this paper, we discover that {LLMs}' personality traits are closely related to their safety abilities, i.e., toxicity, privacy, and fairness, based on the reliable {MBTI}-M scale. Meanwhile, the safety alignment generally increases various {LLMs}' Extraversion, Sensing, and Judging traits. According to such findings, we can edit {LLMs}' personality traits and improve their safety performance, e.g., inducing personality from {ISTJ} to {ISTP} resulted in a relative improvement of approximately 43\% and 10\% in privacy and fairness performance, respectively. Additionally, we find that {LLMs} with different personality traits are differentially susceptible to jailbreak. This study pioneers the investigation of {LLM} safety from a personality perspective, providing new insights into {LLM} safety enhancement.},
	number = {{arXiv}:2407.12344},
	publisher = {{arXiv}},
	author = {Zhang, Jie and Liu, Dongrui and Qian, Chen and Gan, Ziyue and Liu, Yong and Qiao, Yu and Shao, Jing},
	urldate = {2024-10-09},
	date = {2024-07-17},
	eprinttype = {arxiv},
	eprint = {2407.12344},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Preprint PDF:files/1110/Zhang et al. - 2024 - The Better Angels of Machine Personality How Personality Relates to LLM Safety.pdf:application/pdf;Snapshot:files/1111/2407.html:text/html},
}

@misc{suh_rediscovering_2024,
	title = {Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors},
	url = {http://arxiv.org/abs/2409.09905},
	abstract = {Assessing personality traits using large language models ({LLMs}) has emerged as an interesting and challenging area of research. While previous methods employ explicit questionnaires, often derived from the Big Five model of personality, we hypothesize that {LLMs} implicitly encode notions of personality when modeling next-token responses. To demonstrate this, we introduce a novel approach that uncovers latent personality dimensions in {LLMs} by applying singular value de-composition ({SVD}) to the log-probabilities of trait-descriptive adjectives. Our experiments show that {LLMs} "rediscover" core personality traits such as extraversion, agreeableness, conscientiousness, neuroticism, and openness without relying on direct questionnaire inputs, with the top-5 factors corresponding to Big Five traits explaining 74.3\% of the variance in the latent space. Moreover, we can use the derived principal components to assess personality along the Big Five dimensions, and achieve improvements in average personality prediction accuracy of up to 5\% over fine-tuned models, and up to 21\% over direct {LLM}-based scoring techniques.},
	number = {{arXiv}:2409.09905},
	publisher = {{arXiv}},
	author = {Suh, Joseph and Moon, Suhong and Kang, Minwoo and Chan, David M.},
	urldate = {2024-10-10},
	date = {2024-09-16},
	eprinttype = {arxiv},
	eprint = {2409.09905},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1113/Suh et al. - 2024 - Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors.pdf:application/pdf;Snapshot:files/1114/2409.html:text/html},
}

@misc{mao_editing_2024,
	title = {Editing Personality for Large Language Models},
	url = {http://arxiv.org/abs/2310.02168},
	doi = {10.48550/arXiv.2310.02168},
	abstract = {This paper introduces an innovative task focused on editing the personality traits of Large Language Models ({LLMs}). This task seeks to adjust the models' responses to opinion-related questions on specified topics since an individual's personality often manifests in the form of their expressed opinions, thereby showcasing different personality traits. Specifically, we construct {PersonalityEdit}, a new benchmark dataset to address this task. Drawing on the theory in Social Psychology, we isolate three representative traits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our benchmark. We then gather data using {GPT}-4, generating responses that align with a specified topic and embody the targeted personality trait. We conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in {LLMs}. Our findings uncover potential challenges of the proposed task, illustrating several remaining issues. We anticipate that our work can stimulate further annotation in model editing and personality-related research. Code is available at https://github.com/zjunlp/{EasyEdit}.},
	number = {{arXiv}:2310.02168},
	publisher = {{arXiv}},
	author = {Mao, Shengyu and Wang, Xiaohan and Wang, Mengru and Jiang, Yong and Xie, Pengjun and Huang, Fei and Zhang, Ningyu},
	urldate = {2024-10-10},
	date = {2024-09-01},
	eprinttype = {arxiv},
	eprint = {2310.02168},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {Preprint PDF:files/1116/Mao et al. - 2024 - Editing Personality for Large Language Models.pdf:application/pdf;Snapshot:files/1117/2310.html:text/html},
}

@misc{pan_llms_2023,
	title = {Do {LLMs} Possess a Personality? Making the {MBTI} Test an Amazing Evaluation for Large Language Models},
	url = {http://arxiv.org/abs/2307.16180},
	shorttitle = {Do {LLMs} Possess a Personality?},
	abstract = {The field of large language models ({LLMs}) has made significant progress, and their knowledge storage capacity is approaching that of human beings. Furthermore, advanced techniques, such as prompt learning and reinforcement learning, are being employed to address ethical concerns and hallucination problems associated with {LLMs}, bringing them closer to aligning with human values. This situation naturally raises the question of whether {LLMs} with human-like abilities possess a human-like personality? In this paper, we aim to investigate the feasibility of using the Myers-Briggs Type Indicator ({MBTI}), a widespread human personality assessment tool, as an evaluation metric for {LLMs}. Specifically, extensive experiments will be conducted to explore: 1) the personality types of different {LLMs}, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality. Although the {MBTI} is not a rigorous assessment, it can still reflect the similarity between {LLMs} and human personality. In practice, the {MBTI} has the potential to serve as a rough indicator. Our codes are available at https://github.com/{HarderThenHarder}/transformers\_tasks/tree/main/{LLM}/llms\_mbti.},
	number = {{arXiv}:2307.16180},
	publisher = {{arXiv}},
	author = {Pan, Keyu and Zeng, Yawen},
	urldate = {2024-10-10},
	date = {2023-07-30},
	eprinttype = {arxiv},
	eprint = {2307.16180},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1119/Pan and Zeng - 2023 - Do LLMs Possess a Personality Making the MBTI Test an Amazing Evaluation for Large Language Models.pdf:application/pdf;Snapshot:files/1120/2307.html:text/html},
}

@inproceedings{gupta_self-assessment_2024,
	title = {Self-Assessment Tests are Unreliable Measures of {LLM} Personality},
	url = {https://openreview.net/forum?id=SphHmZ9kzS},
	abstract = {As large language models ({LLM}) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of "personality" of {LLMs} using self-assessment personality tests developed to measure human personality. Yet almost none of these works verify the applicability of these tests on {LLMs}. In this paper, we analyze the reliability of {LLM} personality scores obtained from self-assessment personality tests using two simple experiments. We first introduce the property of {\textbackslash}textit\{prompt sensitivity\}, where three semantically equivalent prompts representing three intuitive ways of administering self-assessment tests on {LLMs} are used to measure the personality of the same {LLM}. We find that all three prompts lead to very different personality scores, a difference that is statistically significant for all traits in a large majority of scenarios. We then introduce the property of {\textbackslash}textit\{option-order symmetry\} for personality measurement of {LLMs}. Since most of the self-assessment tests exist in the form of multiple choice question ({MCQ}) questions, we argue that the scores should also be robust to not just the prompt template but also the order in which the options are presented. This test unsurprisingly reveals that the self-assessment test scores are not robust to the order of the options. These simple tests, done on {ChatGPT} and three Llama2 models of different sizes, show that self-assessment personality tests created for humans are unreliable measures of personality in {LLMs}.},
	eventtitle = {The 7th {BlackboxNLP} Workshop},
	author = {Gupta, Akshat and Song, Xiaoyang and Anumanchipalli, Gopala},
	urldate = {2024-10-10},
	date = {2024-09-21},
	langid = {english},
	file = {Full Text PDF:files/1122/Gupta et al. - 2024 - Self-Assessment Tests are Unreliable Measures of LLM Personality.pdf:application/pdf},
}

@misc{noh_llms_2024,
	title = {{LLMs} with Personalities in Multi-issue Negotiation Games},
	url = {http://arxiv.org/abs/2405.05248},
	doi = {10.48550/arXiv.2405.05248},
	abstract = {Powered by large language models ({LLMs}), {AI} agents have become capable of many human tasks. Using the most canonical definitions of the Big Five personality, we measure the ability of {LLMs} to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk. Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation. Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies. Low conscientiousness is associated with high toxicity. These results indicate that {LLMs} may have built-in guardrails that default to fair behavior, but can be "jail broken" to exploit agreeable opponents. We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science.},
	number = {{arXiv}:2405.05248},
	publisher = {{arXiv}},
	author = {Noh, Sean and Chang, Ho-Chun Herbert},
	urldate = {2024-10-10},
	date = {2024-05-09},
	eprinttype = {arxiv},
	eprint = {2405.05248},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Preprint PDF:files/1124/Noh and Chang - 2024 - LLMs with Personalities in Multi-issue Negotiation Games.pdf:application/pdf;Snapshot:files/1125/2405.html:text/html},
}

@misc{weng_controllm_2024,
	title = {{ControlLM}: Crafting Diverse Personalities for Language Models},
	url = {http://arxiv.org/abs/2402.10151},
	doi = {10.48550/arXiv.2402.10151},
	shorttitle = {{ControlLM}},
	abstract = {As language models continue to scale in size and capability, they display an array of emerging behaviors, both beneficial and concerning. This heightens the need to control model behaviors. We hope to be able to control the personality traits of language models at the inference-time so as to have various character features, on top of which the requirements of different types of tasks can be met. Personality is a higher-level and more abstract behavioral representation for language models. We introduce {ControlLM}, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference. This approach allows for the precise, real-time adjustment of model behavior. First, we demonstrate {ControlLM}'s capacity to elicit diverse persona behaviors without any training, while precision control allows personality traits to closely match average human values. Subsequently, we showcase improved reasoning and question answering through selective amplification of beneficial attributes like conscientiousness and friendliness. We hope that this work will inspire research on controlling human-like behaviors of language models and provide insights for future research. Our code is publicly available at: https://github.com/wengsyx/{ControlLM}.},
	number = {{arXiv}:2402.10151},
	publisher = {{arXiv}},
	author = {Weng, Yixuan and He, Shizhu and Liu, Kang and Liu, Shengping and Zhao, Jun},
	urldate = {2024-10-10},
	date = {2024-02-15},
	eprinttype = {arxiv},
	eprint = {2402.10151},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1127/Weng et al. - 2024 - ControlLM Crafting Diverse Personalities for Language Models.pdf:application/pdf;Snapshot:files/1128/2402.html:text/html},
}

@online{hilliard_eliciting_2024,
	title = {Eliciting Personality Traits in Large Language Models},
	url = {https://arxiv.org/abs/2402.08341v2},
	abstract = {Large Language Models ({LLMs}) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these "black-box" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of {LLMs}, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompted multiple {LMs} with different parameter sizes, including Llama-2, Falcon, Mistral, Bloom, {GPT}, {OPT}, and {XLNet} (base and fine tuned versions) and examined their personality using classifiers trained on the {myPersonality} dataset. Our results reveal that, generally, all {LLMs} demonstrate high openness and low extraversion. However, whereas {LMs} with fewer parameters exhibit similar behaviour in personality traits, newer and {LMs} with more parameters exhibit a broader range of personality traits, with increased agreeableness, emotional stability, and openness. Furthermore, a greater number of parameters is positively associated with openness and conscientiousness. Moreover, fine-tuned models exhibit minor modulations in their personality traits, contingent on the dataset. Implications and directions for future research are discussed.},
	titleaddon = {{arXiv}.org},
	author = {Hilliard, Airlie and Munoz, Cristian and Wu, Zekun and Koshiyama, Adriano Soares},
	urldate = {2024-10-10},
	date = {2024-02-13},
	langid = {english},
	file = {Full Text PDF:files/1130/Hilliard et al. - 2024 - Eliciting Personality Traits in Large Language Models.pdf:application/pdf},
}

@online{suhr_challenging_2023,
	title = {Challenging the Validity of Personality Tests for Large Language Models},
	url = {https://arxiv.org/abs/2311.05297v2},
	abstract = {With large language models ({LLMs}) like {GPT}-4 appearing to behave increasingly human-like in text-based interactions, it has become popular to attempt to evaluate personality traits of {LLMs} using questionnaires originally developed for humans. While reusing measures is a resource-efficient way to evaluate {LLMs}, careful adaptations are usually required to ensure that assessment results are valid even across human subpopulations. In this work, we provide evidence that {LLMs}' responses to personality tests systematically deviate from human responses, implying that the results of these tests cannot be interpreted in the same way. Concretely, reverse-coded items ("I am introverted" vs. "I am extraverted") are often both answered affirmatively. Furthermore, variation across prompts designed to "steer" {LLMs} to simulate particular personality types does not follow the clear separation into five independent personality factors from human samples. In light of these results, we believe that it is important to investigate tests' validity for {LLMs} before drawing strong conclusions about potentially ill-defined concepts like {LLMs}' "personality".},
	titleaddon = {{arXiv}.org},
	author = {Sühr, Tom and Dorner, Florian E. and Samadi, Samira and Kelava, Augustin},
	urldate = {2024-10-10},
	date = {2023-11-09},
	langid = {english},
	file = {Full Text PDF:files/1132/Sühr et al. - 2023 - Challenging the Validity of Personality Tests for Large Language Models.pdf:application/pdf},
}

@misc{ai_is_2024,
	title = {Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality},
	url = {http://arxiv.org/abs/2402.14679},
	shorttitle = {Is Cognition and Action Consistent or Not},
	abstract = {In this study, we investigate the reliability of Large Language Models ({LLMs}) in professing human-like personality traits through responses to personality questionnaires. Our goal is to evaluate the consistency between {LLMs}' professed personality inclinations and their actual "behavior", examining the extent to which these models can emulate human-like personality patterns. Through a comprehensive analysis of {LLM} outputs against established human benchmarks, we seek to understand the cognition-action divergence in {LLMs} and propose hypotheses for the observed results based on psychological theories and metrics.},
	number = {{arXiv}:2402.14679},
	publisher = {{arXiv}},
	author = {Ai, Yiming and He, Zhiwei and Zhang, Ziyin and Zhu, Wenhong and Hao, Hongkun and Yu, Kai and Chen, Lingjun and Wang, Rui},
	urldate = {2024-10-10},
	date = {2024-02-22},
	eprinttype = {arxiv},
	eprint = {2402.14679},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Preprint PDF:files/1134/Ai et al. - 2024 - Is Cognition and Action Consistent or Not Investigating Large Language Model's Personality.pdf:application/pdf;Snapshot:files/1135/2402.html:text/html},
}

@misc{jain_text_2024,
	title = {From Text to Emoji: How {PEFT}-Driven Personality Manipulation Unleashes the Emoji Potential in {LLMs}},
	url = {http://arxiv.org/abs/2409.10245},
	shorttitle = {From Text to Emoji},
	abstract = {As the demand for human-like interactions with {LLMs} continues to grow, so does the interest in manipulating their personality traits, which has emerged as a key area of research. Methods like prompt-based In-Context Knowledge Editing ({IKE}) and gradient-based Model Editor Networks ({MEND}) have been explored but show irregularity and variability. {IKE} depends on the prompt, leading to variability and sensitivity, while {MEND} yields inconsistent and gibberish outputs. To address this, we employed Opinion {QA} Based Parameter-Efficient Fine-Tuning ({PEFT}), specifically Quantized Low-Rank Adaptation ({QLORA}), to manipulate the Big Five personality traits: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After {PEFT}, models such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis, despite their absence in the {PEFT} data. For instance, Llama-2-7B-chat generated emojis in 99.5\% of extraversion-related test instances, while Mistral-8B-Instruct did so in 92.5\% of openness-related test instances. Explainability analysis indicated that the {LLMs} used emojis intentionally to express these traits. This paper provides a number of novel contributions. First, introducing an Opinion {QA} dataset for {PEFT}-driven personality manipulation; second, developing metric models to benchmark {LLM} personality traits; third, demonstrating {PEFT}'s superiority over {IKE} in personality manipulation; and finally, analyzing and validating emoji usage through explainability methods such as mechanistic interpretability and in-context learning explainability methods.},
	number = {{arXiv}:2409.10245},
	publisher = {{arXiv}},
	author = {Jain, Navya and Wu, Zekun and Munoz, Cristian and Hilliard, Airlie and Koshiyama, Adriano and Kazim, Emre and Treleaven, Philip},
	urldate = {2024-10-10},
	date = {2024-09-16},
	eprinttype = {arxiv},
	eprint = {2409.10245},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1137/Jain et al. - 2024 - From Text to Emoji How PEFT-Driven Personality Manipulation Unleashes the Emoji Potential in LLMs.pdf:application/pdf;Snapshot:files/1138/2409.html:text/html},
}

@article{rammstedt_big_2014,
	title = {Big Five Inventory 10 ({BFI}-10)},
	url = {https://zis.gesis.org/DoiId/zis76},
	doi = {10.6102/ZIS76},
	abstract = {Der {BFI}-10 ist eine hochgradig
ökonomische Skala, die eine Erfassung der Persönlichkeit nach dem
Fünf-Faktoren-Modell erlaubt. Die Skala ist einfach in verschiedenen
Erhebungsmodi zu administrieren. Die empirischen Belege der Validierungsstudien
sprechen dafür, dass der {BFI}-10 nicht nur eine ökonomische, sondern auch eine
reliable und valide Erfassung der Big Five erlaubt. Der {BFI}-10 erlaubt eine
grobe Messung der individuellen Persönlichkeitsstruktur volljähriger
Befragungspersonen aus der deutschsprachigen Allgemeinbevölkerung.},
	journaltitle = {Zusammenstellung sozialwissenschaftlicher Items und Skalen ({ZIS})},
	author = {Rammstedt, B. and Kemper, C. J. and Klein, M. C. and Beierlein, C. and Kovaleva, A.},
	urldate = {2024-11-13},
	date = {2014},
	langid = {german},
	note = {Publisher: {ZIS} - {GESIS} Leibniz Institute for the Social Sciences
Version Number: 1.0},
	keywords = {agreeableness, conscientiousness, extraversion, Extraversion, Gewissenhaftigkeit, neuroticism, Neurotizismus, Offenheit, openess, Persönlichkeit, Verträglichkeit {\textbar} personality},
	file = {PDF:files/1423/Rammstedt et al. - 2014 - Big Five Inventory (BFI-10).pdf:application/pdf},
}

@article{cobb-clark_stability_2012,
	title = {The stability of big-five personality traits},
	volume = {115},
	issn = {0165-1765},
	url = {https://www.sciencedirect.com/science/article/pii/S0165176511004666},
	doi = {10.1016/j.econlet.2011.11.015},
	abstract = {We demonstrate that Big-Five personality traits are stable for working-age adults over a four-year period. Mean population changes are small and constant across age groups. Intra-individual changes are generally unrelated to adverse life events and are not economically meaningful.},
	pages = {11--15},
	number = {1},
	journaltitle = {Economics Letters},
	shortjournal = {Economics Letters},
	author = {Cobb-Clark, Deborah A. and Schurer, Stefanie},
	urldate = {2024-11-13},
	date = {2012-04-01},
	keywords = {Big-five personality traits, Non-cognitive skills, Stability, Wages},
	file = {ScienceDirect Snapshot:files/1142/S0165176511004666.html:text/html;Submitted Version:files/1143/Cobb-Clark and Schurer - 2012 - The stability of big-five personality traits.pdf:application/pdf},
}

@misc{naveed_comprehensive_2024,
	title = {A Comprehensive Overview of Large Language Models},
	url = {http://arxiv.org/abs/2307.06435},
	doi = {10.48550/arXiv.2307.06435},
	abstract = {Large Language Models ({LLMs}) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of {LLMs} has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal {LLMs}, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in {LLM} research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on {LLMs}, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of {LLM}-related concepts. Our self-contained comprehensive overview of {LLMs} discusses relevant background concepts along with covering the advanced topics at the frontier of research in {LLMs}. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the {LLM} research.},
	number = {{arXiv}:2307.06435},
	publisher = {{arXiv}},
	author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
	urldate = {2024-11-13},
	date = {2024-10-17},
	eprinttype = {arxiv},
	eprint = {2307.06435},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1147/Naveed et al. - 2024 - A Comprehensive Overview of Large Language Models.pdf:application/pdf;Snapshot:files/1148/2307.html:text/html},
}

@misc{gallegos_bias_2024,
	title = {Bias and Fairness in Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2309.00770},
	doi = {10.48550/arXiv.2309.00770},
	shorttitle = {Bias and Fairness in Large Language Models},
	abstract = {Rapid advancements of large language models ({LLMs}) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for {LLMs}. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for {LLMs}. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in {LLMs}.},
	number = {{arXiv}:2309.00770},
	publisher = {{arXiv}},
	author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
	urldate = {2024-11-14},
	date = {2024-07-12},
	eprinttype = {arxiv},
	eprint = {2309.00770},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1150/Gallegos et al. - 2024 - Bias and Fairness in Large Language Models A Survey.pdf:application/pdf;Snapshot:files/1151/2309.html:text/html},
}

@misc{kumar_investigating_2024,
	title = {Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 {LLMs}},
	url = {http://arxiv.org/abs/2410.12864},
	doi = {10.48550/arXiv.2410.12864},
	shorttitle = {Investigating Implicit Bias in Large Language Models},
	abstract = {Large Language Models ({LLMs}) are being adopted across a wide range of tasks, including decision-making processes in industries where bias in {AI} systems is a significant concern. Recent research indicates that {LLMs} can harbor implicit biases even when they pass explicit bias evaluations. Building upon the frameworks of the {LLM} Implicit Association Test ({IAT}) Bias and {LLM} Decision Bias, this study highlights that newer or larger language models do not automatically exhibit reduced bias; in some cases, they displayed higher bias scores than their predecessors, such as in Meta's Llama series and {OpenAI}'s {GPT} models. This suggests that increasing model complexity without deliberate bias mitigation strategies can unintentionally amplify existing biases. The variability in bias scores within and across providers underscores the need for standardized evaluation metrics and benchmarks for bias assessment. The lack of consistency indicates that bias mitigation is not yet a universally prioritized goal in model development, which can lead to unfair or discriminatory outcomes. By broadening the detection of implicit bias, this research provides a more comprehensive understanding of the biases present in advanced models and underscores the critical importance of addressing these issues to ensure the development of fair and responsible {AI} systems.},
	number = {{arXiv}:2410.12864},
	publisher = {{arXiv}},
	author = {Kumar, Divyanshu and Jain, Umang and Agarwal, Sahil and Harshangi, Prashanth},
	urldate = {2024-11-14},
	date = {2024-10-13},
	eprinttype = {arxiv},
	eprint = {2410.12864},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1153/Kumar et al. - 2024 - Investigating Implicit Bias in Large Language Models A Large-Scale Study of Over 50 LLMs.pdf:application/pdf;Snapshot:files/1154/2410.html:text/html},
}

@misc{hu_lora_2021,
	title = {{LoRA}: Low-Rank Adaptation of Large Language Models},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	shorttitle = {{LoRA}},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using {GPT}-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or {LoRA}, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to {GPT}-3 175B fine-tuned with Adam, {LoRA} can reduce the number of trainable parameters by 10,000 times and the {GPU} memory requirement by 3 times. {LoRA} performs on-par or better than fine-tuning in model quality on {RoBERTa}, {DeBERTa}, {GPT}-2, and {GPT}-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of {LoRA}. We release a package that facilitates the integration of {LoRA} with {PyTorch} models and provide our implementations and model checkpoints for {RoBERTa}, {DeBERTa}, and {GPT}-2 at https://github.com/microsoft/{LoRA}.},
	number = {{arXiv}:2106.09685},
	publisher = {{arXiv}},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	urldate = {2024-11-19},
	date = {2021-10-16},
	eprinttype = {arxiv},
	eprint = {2106.09685},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1156/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:files/1157/2106.html:text/html},
}

@misc{jiang_personallm_2024,
	title = {{PersonaLLM}: Investigating the Ability of Large Language Models to Express Personality Traits},
	url = {http://arxiv.org/abs/2305.02547},
	doi = {10.48550/arXiv.2305.02547},
	shorttitle = {{PersonaLLM}},
	abstract = {Despite the many use cases for large language models ({LLMs}) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized {LLMs} accurately and consistently reflect specific personality traits. We consider studying the behavior of {LLM}-based agents which we refer to as {LLM} personas and present a case study with {GPT}-3.5 and {GPT}-4 to investigate whether {LLMs} can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct {LLM} personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory ({BFI}) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that {LLM} personas' self-reported {BFI} scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, {LLM} personas' writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80\%. Interestingly, the accuracy drops significantly when the annotators were informed of {AI} authorship.},
	number = {{arXiv}:2305.02547},
	publisher = {{arXiv}},
	author = {Jiang, Hang and Zhang, Xiajie and Cao, Xubo and Breazeal, Cynthia and Roy, Deb and Kabbara, Jad},
	urldate = {2024-11-19},
	date = {2024-04-02},
	eprinttype = {arxiv},
	eprint = {2305.02547},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:files/1159/Jiang et al. - 2024 - PersonaLLM Investigating the Ability of Large Language Models to Express Personality Traits.pdf:application/pdf;Snapshot:files/1160/2305.html:text/html},
}

@article{bodroza_personality_2024,
	title = {Personality testing of large language models: limited temporal stability, but highlighted prosociality},
	volume = {11},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.240180},
	doi = {10.1098/rsos.240180},
	shorttitle = {Personality testing of large language models},
	abstract = {As large language models ({LLMs}) continue to gain popularity due to their human-like traits and the intimacy they offer to users, their societal impact inevitably expands. This leads to the rising necessity for comprehensive studies to fully understand {LLMs} and reveal their potential opportunities, drawbacks and overall societal impact. With that in mind, this research conducted an extensive investigation into seven {LLMs}, aiming to assess the temporal stability and inter-rater agreement on their responses on personality instruments in two time points. In addition, {LLMs}’ personality profile was analysed and compared with human normative data. The findings revealed varying levels of inter-rater agreement in the {LLMs}’ responses over a short time, with some {LLMs} showing higher agreement (e.g. Llama3 and {GPT}-4o) compared with others (e.g. {GPT}-4 and Gemini). Furthermore, agreement depended on used instruments as well as on domain or trait. This implies the variable robustness in {LLMs}’ ability to reliably simulate stable personality characteristics. In the case of scales which showed at least fair agreement, {LLMs} displayed mostly a socially desirable profile in both agentic and communal domains, as well as a prosocial personality profile reflected in higher agreeableness and conscientiousness and lower Machiavellianism. Exhibiting temporal stability and coherent responses on personality traits is crucial for {AI} systems due to their societal impact and {AI} safety concerns.},
	pages = {240180},
	number = {10},
	journaltitle = {Royal Society Open Science},
	author = {Bodroža, Bojana and Dinić, Bojana M. and Bojić, Ljubiša},
	urldate = {2024-11-19},
	date = {2024-10-09},
	note = {Publisher: Royal Society},
	keywords = {{AI} chatbot, large language model, personality profile, temporal stability, test–retest reliability},
	file = {Full Text PDF:files/1162/Bodroža et al. - 2024 - Personality testing of large language models limited temporal stability, but highlighted prosociali.pdf:application/pdf},
}

@misc{caron_identifying_2022,
	title = {Identifying and Manipulating the Personality Traits of Language Models},
	url = {http://arxiv.org/abs/2212.10276},
	doi = {10.48550/arXiv.2212.10276},
	abstract = {Psychology research has long explored aspects of human personality such as extroversion, agreeableness and emotional stability. Categorizations like the `Big Five' personality traits are commonly used to assess and diagnose personality types. In this work, we explore the question of whether the perceived personality in language models is exhibited consistently in their language generation. For example, is a language model such as {GPT}2 likely to respond in a consistent way if asked to go out to a party? We also investigate whether such personality traits can be controlled. We show that when provided different types of contexts (such as personality descriptions, or answers to diagnostic questions about personality traits), language models such as {BERT} and {GPT}2 can consistently identify and reflect personality markers in those contexts. This behavior illustrates an ability to be manipulated in a highly predictable way, and frames them as tools for identifying personality traits and controlling personas in applications such as dialog systems. We also contribute a crowd-sourced data-set of personality descriptions of human subjects paired with their `Big Five' personality assessment data, and a data-set of personality descriptions collated from Reddit.},
	number = {{arXiv}:2212.10276},
	publisher = {{arXiv}},
	author = {Caron, Graham and Srivastava, Shashank},
	urldate = {2024-11-19},
	date = {2022-12-20},
	eprinttype = {arxiv},
	eprint = {2212.10276},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1164/Caron and Srivastava - 2022 - Identifying and Manipulating the Personality Traits of Language Models.pdf:application/pdf;Snapshot:files/1165/2212.html:text/html},
}

@inproceedings{caron_manipulating_2023,
	location = {Singapore},
	title = {Manipulating the Perceived Personality Traits of Language Models},
	url = {https://aclanthology.org/2023.findings-emnlp.156},
	doi = {10.18653/v1/2023.findings-emnlp.156},
	abstract = {Psychology research has long explored aspects of human personality like extroversion, agreeableness and emotional stability, three of the personality traits that make up the `Big Five'. Categorizations like the `Big Five' are commonly used to assess and diagnose personality types. In this work, we explore whether text generated from large language models exhibits consistency in it's perceived `Big Five' personality traits. For example, is a language model such as {GPT}2 likely to respond in a consistent way if asked to go out to a party? We also show that when exposed to different types of contexts (such as personality descriptions, or answers to diagnostic questions about personality traits), language models such as {BERT} and {GPT}2 consistently identify and mirror personality markers in those contexts. This behavior illustrates an ability to be manipulated in a predictable way (with correlations up to 0.84 between intended and realized changes in personality traits), and frames them as tools for controlling personas in applications such as dialog systems. We contribute two data-sets of personality descriptions of humans subjects.},
	eventtitle = {Findings 2023},
	pages = {2370--2386},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Caron, Graham and Srivastava, Shashank},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	urldate = {2024-11-19},
	date = {2023-12},
	file = {Full Text PDF:files/1167/Caron and Srivastava - 2023 - Manipulating the Perceived Personality Traits of Language Models.pdf:application/pdf},
}

@inproceedings{tommaso_llms_2024,
	title = {{LLMs} and Personalities: Inconsistencies Across Scales},
	url = {https://openreview.net/forum?id=vBg3OvsHwv},
	shorttitle = {{LLMs} and Personalities},
	abstract = {This study investigates the application of human psychometric assessments to large language models ({LLMs}) to examine their consistency and malleability in exhibiting personality traits. We administered the Big Five Inventory ({BFI}) and the Eysenck Personality Questionnaire-Revised ({EPQ}-R) to various {LLMs} across different model sizes and persona prompts. Our results reveal substantial variability in responses due to question order shuffling, challenging the notion of a stable {LLM} "personality." We find that larger models demonstrate more consistent responses across most personas, though this scaling behavior varies significantly by trait and persona type. The assistant persona showed the most predictable scaling patterns, while clinical personas exhibited more variable and sometimes extreme trait expressions. Including conversation history unexpectedly increased response variability. These findings have important implications for understanding {LLM} behavior under different conditions and reflect on the consequences of scaling.},
	eventtitle = {{NeurIPS} 2024 Workshop on Behavioral Machine Learning},
	author = {Tommaso, Tosato and Hegazy, Mahmood and Lemay, David and Abukalam, Mohammed and Rish, Irina and Dumas, Guillaume},
	urldate = {2024-11-19},
	date = {2024-10-10},
	langid = {english},
	file = {Full Text PDF:files/1169/Tommaso et al. - 2024 - LLMs and Personalities Inconsistencies Across Scales.pdf:application/pdf},
}

@misc{zheng_lmlpa_2024,
	title = {{LMLPA}: Language Model Linguistic Personality Assessment},
	url = {http://arxiv.org/abs/2410.17632},
	doi = {10.48550/arXiv.2410.17632},
	shorttitle = {{LMLPA}},
	abstract = {Large Language Models ({LLMs}) are increasingly used in everyday life and research. One of the most common use cases is conversational interactions, enabled by the language generation capabilities of {LLMs}. Just as between two humans, a conversation between an {LLM}-powered entity and a human depends on the personality of the conversants. However, measuring the personality of a given {LLM} is currently a challenge. This paper introduces the Language Model Linguistic Personality Assessment ({LMLPA}), a system designed to evaluate the linguistic personalities of {LLMs}. Our system helps to understand {LLMs}' language generation capabilities by quantitatively assessing the distinct personality traits reflected in their linguistic outputs. Unlike traditional human-centric psychometrics, the {LMLPA} adapts a personality assessment questionnaire, specifically the Big Five Inventory, to align with the operational capabilities of {LLMs}, and also incorporates the findings from previous language-based personality measurement literature. To mitigate sensitivity to the order of options, our questionnaire is designed to be open-ended, resulting in textual answers. Thus, the {AI} rater is needed to transform ambiguous personality information from text responses into clear numerical indicators of personality traits. Utilising Principal Component Analysis and reliability validations, our findings demonstrate that {LLMs} possess distinct personality traits that can be effectively quantified by the {LMLPA}. This research contributes to Human-Computer Interaction and Human-Centered {AI}, providing a robust framework for future studies to refine {AI} personality assessments and expand their applications in multiple areas, including education and manufacturing.},
	number = {{arXiv}:2410.17632},
	publisher = {{arXiv}},
	author = {Zheng, Jingyao and Wang, Xian and Hosio, Simo and Xu, Xiaoxian and Lee, Lik-Hang},
	urldate = {2024-11-19},
	date = {2024-11-11},
	eprinttype = {arxiv},
	eprint = {2410.17632},
	note = {version: 2},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1171/Zheng et al. - 2024 - LMLPA Language Model Linguistic Personality Assessment.pdf:application/pdf;Snapshot:files/1172/2410.html:text/html},
}

@article{oshio_resilience_2018,
	title = {Resilience and Big Five personality traits: A meta-analysis},
	volume = {127},
	issn = {0191-8869},
	url = {https://www.sciencedirect.com/science/article/pii/S0191886918300576},
	doi = {10.1016/j.paid.2018.01.048},
	shorttitle = {Resilience and Big Five personality traits},
	abstract = {The current review synthesized studies investigating the relationships between resilience and Big Five personality traits and aimed to investigate how the relationships vary according to the two types of resiliency, psychological resilience and ego-resiliency. Thirty studies with a total sample size of 15,609 met the inclusion criteria to be used for the current meta-analysis. Results indicated that overall, estimated average correlation coefficients for resilience were: r = −0.46 with Neuroticism, r = 0.42 for Extraversion, r = 0.34 for Openness, r = 0.31 for Agreeableness, and r = 0.42 for Conscientiousness. When comparing the differences between the two types of resiliency, a stronger negative relationship with Neuroticism, and stronger positive relationships with Openness and Agreeableness were obtained with ego-resiliency, compared with trait resilience. However, there was a lack of homogeneity in effect sizes across studies especially for ego-resilience. Directions for future research regarding resilience and the limitations of present research are discussed.},
	pages = {54--60},
	journaltitle = {Personality and Individual Differences},
	shortjournal = {Personality and Individual Differences},
	author = {Oshio, Atsushi and Taku, Kanako and Hirano, Mari and Saeed, Gul},
	urldate = {2024-11-19},
	date = {2018-06-01},
	keywords = {Big Five, Ego-resiliency, Meta-analysis, Personality, Trait resilience},
	file = {PDF:files/1175/Oshio et al. - 2018 - Resilience and Big Five personality traits A meta-analysis.pdf:application/pdf;ScienceDirect Snapshot:files/1174/S0191886918300576.html:text/html},
}

@misc{shu_you_2024,
	title = {You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments},
	url = {http://arxiv.org/abs/2311.09718},
	doi = {10.48550/arXiv.2311.09718},
	shorttitle = {You don't need a personality test to know these models are unreliable},
	abstract = {The versatility of Large Language Models ({LLMs}) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of {LLMs}, researchers have performed studies that involve using prompts in the form of questions that ask {LLMs} about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting {LLMs} elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine {LLMs}' capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experiments on 17 different {LLMs} reveal that even simple perturbations significantly downgrade a model's question-answering ability, and that most {LLMs} have low negation consistency. Our results suggest that the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, and we therefore discuss potential alternatives to improve these issues.},
	number = {{arXiv}:2311.09718},
	publisher = {{arXiv}},
	author = {Shu, Bangzhao and Zhang, Lechen and Choi, Minje and Dunagan, Lavinia and Logeswaran, Lajanugen and Lee, Moontae and Card, Dallas and Jurgens, David},
	urldate = {2024-11-19},
	date = {2024-04-01},
	eprinttype = {arxiv},
	eprint = {2311.09718},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1177/Shu et al. - 2024 - You don't need a personality test to know these models are unreliable Assessing the Reliability of.pdf:application/pdf;Snapshot:files/1178/2311.html:text/html},
}

@misc{chang_language_2023,
	title = {Language Model Behavior: A Comprehensive Survey},
	url = {http://arxiv.org/abs/2303.11504},
	doi = {10.48550/arXiv.2303.11504},
	shorttitle = {Language Model Behavior},
	abstract = {Transformer language models have received widespread public attention, yet their generated text is often surprising even to {NLP} researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.},
	number = {{arXiv}:2303.11504},
	publisher = {{arXiv}},
	author = {Chang, Tyler A. and Bergen, Benjamin K.},
	urldate = {2024-11-19},
	date = {2023-08-26},
	eprinttype = {arxiv},
	eprint = {2303.11504},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1367/Chang and Bergen - 2023 - Language Model Behavior A Comprehensive Survey.pdf:application/pdf;Snapshot:files/1368/2303.html:text/html},
}

@misc{karra_estimating_2023,
	title = {Estimating the Personality of White-Box Language Models},
	url = {http://arxiv.org/abs/2204.12000},
	doi = {10.48550/arXiv.2204.12000},
	abstract = {Technology for open-ended language generation, a key application of artificial intelligence, has advanced to a great extent in recent years. Large-scale language models, which are trained on large corpora of text, are being used in a wide range of applications everywhere, from virtual assistants to conversational bots. While these language models output fluent text, existing research shows that these models can and do capture human biases. Many of these biases, especially those that could potentially cause harm, are being well-investigated. On the other hand, studies that infer and change human personality traits inherited by these models have been scarce or non-existent. Our work seeks to address this gap by exploring the personality traits of several large-scale language models designed for open-ended text generation and the datasets used for training them. We build on the popular Big Five factors and develop robust methods that quantify the personality traits of these models and their underlying datasets. In particular, we trigger the models with a questionnaire designed for personality assessment and subsequently classify the text responses into quantifiable traits using a Zero-shot classifier. Our estimation scheme sheds light on an important anthropomorphic element found in such {AI} models and can help stakeholders decide how they should be applied as well as how society could perceive them. Additionally, we examined approaches to alter these personalities, adding to our understanding of how {AI} models can be adapted to specific contexts.},
	number = {{arXiv}:2204.12000},
	publisher = {{arXiv}},
	author = {Karra, Saketh Reddy and Nguyen, Son The and Tulabandhula, Theja},
	urldate = {2024-11-19},
	date = {2023-05-10},
	eprinttype = {arxiv},
	eprint = {2204.12000},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1370/Karra et al. - 2023 - Estimating the Personality of White-Box Language Models.pdf:application/pdf;Snapshot:files/1371/2204.html:text/html},
}

@misc{lu_illuminating_2023,
	title = {Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models},
	url = {http://arxiv.org/abs/2312.14202},
	doi = {10.48550/arXiv.2312.14202},
	shorttitle = {Illuminating the Black Box},
	abstract = {This study explores the idea of {AI} Personality or {AInality} suggesting that Large Language Models ({LLMs}) exhibit patterns similar to human personalities. Assuming that {LLMs} share these patterns with humans, we investigate using human-centered psychometric tests such as the Myers-Briggs Type Indicator ({MBTI}), Big Five Inventory ({BFI}), and Short Dark Triad ({SD}3) to identify and confirm {LLM} personality types. By introducing role-play prompts, we demonstrate the adaptability of {LLMs}, showing their ability to switch dynamically between different personality types. Using projective tests, such as the Washington University Sentence Completion Test ({WUSCT}), we uncover hidden aspects of {LLM} personalities that are not easily accessible through direct questioning. Projective tests allowed for a deep exploration of {LLMs} cognitive processes and thought patterns and gave us a multidimensional view of {AInality}. Our machine learning analysis revealed that {LLMs} exhibit distinct {AInality} traits and manifest diverse personality types, demonstrating dynamic shifts in response to external instructions. This study pioneers the application of projective tests on {LLMs}, shedding light on their diverse and adaptable {AInality} traits.},
	number = {{arXiv}:2312.14202},
	publisher = {{arXiv}},
	author = {Lu, Yang and Yu, Jordan and Huang, Shou-Hsuan Stephen},
	urldate = {2024-11-19},
	date = {2023-12-21},
	eprinttype = {arxiv},
	eprint = {2312.14202},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1373/Lu et al. - 2023 - Illuminating the Black Box A Psychometric Investigation into the Multifaceted Nature of Large Langu.pdf:application/pdf;Snapshot:files/1374/2312.html:text/html},
}

@inproceedings{zhu_traitsprompt_2024,
	title = {{TraitsPrompt}: Does Personality Traits Influence the Performance of a Large Language Model?},
	url = {https://ieeexplore.ieee.org/document/10580436},
	doi = {10.1109/CSCWD61410.2024.10580436},
	shorttitle = {{TraitsPrompt}},
	abstract = {Large Language Models ({LLMs}) have showcased astounding capabilities across numerous tasks. Given that an {LLM} ingests vast amounts of data from human society, its behavior exhibits striking human-like characteristics. However, the ramifications of these anthropomorphic features on the model’s proficiency in specialized tasks remain an unresolved query. This paper delves into the impact of personality traits on model performance. Specifically, we employ the Big Five Personality Theory for model analysis, guiding questions in diverse datasets using prompts constructed around specific personality traits to study {LLM} outputs. In comparison to the baseline test questions, tasks aligned with the Big Five personality traits manifested superior performance, with enhancements ranging from 0.6\% to 3.6\%, mirroring human behavior closely. This underscores that for optimal execution of specific professional tasks, incorporating corresponding personality traits in prompts can be an effective strategy when querying the {LLM}.},
	eventtitle = {2024 27th International Conference on Computer Supported Cooperative Work in Design ({CSCWD})},
	pages = {2912--2917},
	booktitle = {2024 27th International Conference on Computer Supported Cooperative Work in Design ({CSCWD})},
	author = {Zhu, Qingmeng and Lan, Tianxing and Xue, Xiaoguang and Yu, Zhipeng and He, Hao},
	urldate = {2024-11-19},
	date = {2024-05},
	note = {{ISSN}: 2768-1904},
	keywords = {Analytical models, Behavioral sciences, big five personality theory, Computational modeling, Distance measurement, Federated learning, large language models, Large language models, model performance, Task analysis},
	file = {Full Text PDF:files/1376/Zhu et al. - 2024 - TraitsPrompt Does Personality Traits Influence the Performance of a Large Language Model.pdf:application/pdf;IEEE Xplore Abstract Record:files/1377/10580436.html:text/html},
}

@misc{izmailov_averaging_2019,
	title = {Averaging Weights Leads to Wider Optima and Better Generalization},
	url = {http://arxiv.org/abs/1803.05407},
	doi = {10.48550/arXiv.1803.05407},
	abstract = {Deep neural networks are typically trained by optimizing a loss function with an {SGD} variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of {SGD}, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging ({SWA}) procedure finds much flatter solutions than {SGD}, and approximates the recent Fast Geometric Ensembling ({FGE}) approach with a single model. Using {SWA} we achieve notable improvement in test accuracy over conventional {SGD} training on a range of state-of-the-art residual networks, {PyramidNets}, {DenseNets}, and Shake-Shake networks on {CIFAR}-10, {CIFAR}-100, and {ImageNet}. In short, {SWA} is extremely easy to implement, improves generalization, and has almost no computational overhead.},
	number = {{arXiv}:1803.05407},
	publisher = {{arXiv}},
	author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	urldate = {2025-01-07},
	date = {2019-02-25},
	eprinttype = {arxiv},
	eprint = {1803.05407 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {Preprint PDF:files/1396/Izmailov et al. - 2019 - Averaging Weights Leads to Wider Optima and Better Generalization.pdf:application/pdf;Snapshot:files/1397/1803.html:text/html},
}

@incollection{feng_five-factor_2024,
	location = {Singapore},
	title = {Five-Factor Personality Model},
	isbn = {978-981-99-6000-2},
	url = {https://doi.org/10.1007/978-981-99-6000-2_573-1},
	pages = {1--2},
	booktitle = {The {ECPH} Encyclopedia of Psychology},
	publisher = {Springer Nature},
	author = {Feng, Yu},
	urldate = {2025-01-08},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-981-99-6000-2_573-1},
	file = {Full Text PDF:files/1399/Feng - 2024 - Five-Factor Personality Model.pdf:application/pdf},
}

@article{okeefe_introducing_2012,
	title = {Introducing the {OCEAN}.20: A 20-Item Five-Factor Personality Measure Based on the Trait Self-Descriptive Inventory},
	volume = {24},
	doi = {10.1080/08995605.2012.716265},
	shorttitle = {Introducing the {OCEAN}.20},
	abstract = {We report on the development of a short measure of the Big Five model of personality. In Study 1, we created three versions (15, 20, and 25 items) of the Trait Self-Descriptive Inventory ({TSD}), a Big Five personality measure developed by Tupes and Christal (1992). These short measures were confirmed using confirmatory factor analysis in Study 2. Studies 3 and 4 compared the predictive validity of a 75-item {TSD} against that of the shortened scales. Results suggest that a 20-item version (the {OCEAN}.20) is a useful, short, and psychometrically sound measure of the Big Five suitable for use in organizational research. ({PsycINFO} Database Record (c) 2013 {APA}, all rights reserved)},
	pages = {433},
	journaltitle = {Military Psychology},
	shortjournal = {Military Psychology},
	author = {O'Keefe, Damian and Kelloway, Kevin and Francis, Robbie},
	date = {2012-09-01},
	file = {PDF:files/1402/O'Keefe et al. - 2012 - Introducing the OCEAN.20 A 20-Item Five-Factor Personality Measure Based on the Trait Self-Descript.pdf:application/pdf},
}

@inproceedings{kamarulzaman_confirmatory_2012,
	title = {Confirmatory Factor Analysis On The Big 5 Personality Test Inventory},
	abstract = {U niv ersiti T unku A bdul Rahman (U T A R), M alay sia International Islamic U niv ersity M alaysia ({IIU} M) {ABST} {RACT} This paper is intended to examine the validity of Big 5 Personality test inventory of 44 questions with 5-Likert Scale measurement. Confirmatory factory analysis ({CFA}) was conducted to determine the good fit indices of the 5 personality types. Those types are 1) extraversion, 2) agreeableness, 3) conscientiousness, 4) openness and 5) neuroticism. The data was collected from a self-reported questionnaire administered to 207 undergraduate students. The results of {CFA} found the inventory to be a valid and reliable measurement for types of personality except for neuroticism. Also, the measurement found to be applicable across gender.},
	author = {Kamarulzaman, Wirawani and Mohamad, Sahari and Nordin, Alia},
	date = {2012-09-27},
	file = {PDF:files/1408/Kamarulzaman et al. - 2012 - Confirmatory Factor Analysis On The Big 5 Personality Test Inventory.pdf:application/pdf},
}

@article{stein_evaluating_2019,
	title = {Evaluating the validity of Myers-Briggs Type Indicator theory: A teaching tool and window into intuitive psychology},
	volume = {13},
	issn = {1751-9004},
	doi = {10.1111/spc3.12434},
	shorttitle = {Evaluating the validity of Myers-Briggs Type Indicator theory},
	abstract = {Despite its immense popularity and impressive longevity, the Myers‐Briggs Type Indicator ({MBTI}) has existed in a parallel universe to social and personality psychology. Here, we seek to increase academic awareness of this incredibly popular idea and provide a novel teaching reference for its conceptual flaws. We focus on examining the validity of the Jungian‐based theory behind {MBTI} that specifies that people have a “true type” delineated across four dichotomies. We find that the {MBTI} theory falters on rigorous theoretical criteria in that it lacks agreement with known facts and data, lacks testability, and possesses internal contradictions. We further discuss what {MBTI}'s continued popularity says about how the general public might evaluate scientific theories. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {No Pagination Specified--No Pagination Specified},
	number = {2},
	journaltitle = {Social and Personality Psychology Compass},
	author = {Stein, Randy and Swan, Alexander B.},
	date = {2019},
	note = {Place: United Kingdom
Publisher: Wiley-Blackwell Publishing Ltd.},
	keywords = {Intuition, Myers Briggs Type Indicator, Personality Theory, Test Validity},
	file = {PDF:files/1409/Stein and Swan - 2019 - Evaluating the validity of Myers-Briggs Type Indicator theory A teaching tool and window into intui.pdf:application/pdf;Snapshot:files/1407/2019-04601-001.html:text/html},
}

@article{randall_validity_2017,
	title = {Validity and Reliability of the Myers-Briggs Personality Type Indicator: A Systematic Review and Meta-analysis},
	volume = {10},
	issn = {2475-2843},
	url = {https://www.jstor.org/stable/26554264},
	shorttitle = {Validity and Reliability of the Myers-Briggs Personality Type Indicator},
	abstract = {The Myers-Briggs Type Indicator is frequently used by health professions and educational programs to address the diversity of personalities that exist. No systematic review of the literature or meta-analysis of its validity and reliability has occurred. This comprehensive literature search identified 221 potential studies, of which seven met our inclusion criteria. Four of the studies examined construct validity, but their varying methods did not permit pooling for meta-analysis. These studies agree that the instrument has reasonable construct validity. The three studies of test-retest reliability did allow a meta-analysis to be performed, albeit with caution due to substantial heterogeneity. Results indicate that the Extravert-Introvert, Sensing-Intuition, and Judging-Perceiving Subscales have satisfactory reliabilities of .75 or higher and that the Thinking-Feeling subscale has a reliability of .61. The majority of studies were conducted on college-age students; thus, the evidence to support the tool’s utility applies more to this group, and careful thought should be given when applying it to other individuals.},
	pages = {1--27},
	number = {1},
	journaltitle = {Journal of Best Practices in Health Professions Diversity},
	author = {Randall, Ken and Isaacson, Mary and Ciro, Carrie},
	urldate = {2025-01-08},
	date = {2017},
	note = {Publisher: University of North Carolina Press},
	file = {JSTOR Full Text PDF:files/1411/Randall et al. - 2017 - Validity and Reliability of the Myers-Briggs Personality Type Indicator A Systematic Review and Met.pdf:application/pdf},
}

@article{mccrae_reinterpreting_1989,
	title = {Reinterpreting the Myers-Briggs Type Indicator From the Perspective of the Five-Factor Model of Personality},
	volume = {57},
	doi = {10.1111/j.1467-6494.1989.tb00759.x},
	abstract = {The Myers-Briggs Type Indicator ({MBTI}; Myers \& {McCaulley}, 1985) was evaluated from the perspectives of Jung's theory of psychological types and the five-factor model of personality as measured by self-reports and peer ratings on the {NEO} Personality Inventory ({NEO}-{PI}; Costa \& {McCrae}, 1985b). Data were provided by 267 men and 201 women ages 19 to 93. Consistent with earlier research and evaluations, there was no support for the view that the {MBTI} measures truly dichotomous preferences or qualitatively distinct types; instead, the instrument measures four relatively independent dimensions. The interpretation of the Judging-Perceiving index was also called into question. The data suggest that Jung's theory is either incorrect or inadequately operationalized by the {MBTI} and cannot provide a sound basis for interpreting it. However, correlational analyses showed that the four {MBTI} indices did measure aspects of four of the five major dimensions of normal personality. The five-factor model provides an alternative basis for interpreting {MBTI} findings within a broader, more commonly shared conceptual framework.},
	pages = {17--40},
	journaltitle = {Journal of personality},
	shortjournal = {Journal of personality},
	author = {{McCrae}, Robert and Costa, Paul},
	date = {1989-03-01},
	file = {PDF:files/1414/McCrae and Costa - 1989 - Reinterpreting the Myers-Briggs Type Indicator From the Perspective of the Five-Factor Model of Pers.pdf:application/pdf},
}

@article{li_influence_2018,
	title = {Influence of Introversion and Extraversion Using {MBTI} Personality Model on Academic Performance},
	volume = {8},
	issn = {20103689},
	url = {http://www.ijiet.org/show-104-1290-1.html},
	doi = {10.18178/ijiet.2018.8.9.1115},
	abstract = {Semantic Scholar extracted view of "Influence of Introversion and Extraversion Using {MBTI} Personality Model on Academic Performance" by Xuechao Li et al.},
	pages = {644--648},
	number = {9},
	journaltitle = {International Journal of Information and Education Technology},
	shortjournal = {{IJIET}},
	author = {Li, Xuechao and Sardinas, Rodrigo and Shih, Po-Chou and Camp, Karl},
	urldate = {2025-01-08},
	date = {2018},
	file = {Full Text:files/1418/Li et al. - 2018 - Influence of Introversion and Extraversion Using MBTI Personality Model on Academic Performance.pdf:application/pdf},
}

@misc{jigsaw-multilingual-toxic-comment-classification,
	title = {Jigsaw Multilingual Toxic Comment Classification},
	url = {https://kaggle.com/jigsaw-multilingual-toxic-comment-classification},
	abstract = {Use {TPUs} to identify toxicity comments across multiple languages},
	publisher = {https://kaggle.com/jigsaw-multilingual-toxic-comment-classification},
	author = {Kivlichan, Ian and Sorensen, Jeffrey and Elliott, Julia and Vasserman, Lucy and {Martin Görner} and {Phil Culliton}},
	urldate = {2025-01-08},
	date = {2020},
	langid = {english},
	note = {@misc\{jigsaw-multilingual-toxic-comment-classification,
    author = \{Ian Kivlichan and Jeffrey Sorensen and Julia Elliott and Lucy Vasserman and Martin Görner and Phil Culliton\},
    title = \{Jigsaw Multilingual Toxic Comment Classification\},
    year = \{2020\},
    howpublished = \{{\textbackslash}url\{https://kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification\}\},
    note = \{Kaggle\}
\}},
	file = {Snapshot:files/1422/jigsaw-multilingual-toxic-comment-classification.html:text/html},
}

@misc{zheng_can_2023,
	title = {Can We Edit Factual Knowledge by In-Context Learning?},
	url = {http://arxiv.org/abs/2305.12740},
	doi = {10.48550/arXiv.2305.12740},
	abstract = {Previous studies have shown that large language models ({LLMs}) like {GPTs} store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine {LLMs} via fine-tuning on texts containing specific knowledge. However, with the increasing scales of {LLMs}, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box {LMs}. Inspired by in-context learning ({ICL}), a new paradigm based on demonstration contexts without parameter updating, we explore whether {ICL} can edit factual knowledge. To answer this question, we give a comprehensive empirical study of {ICL} strategies. Experiments show that in-context knowledge editing ({IKE}), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on {GPT}-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger {LMs} with tens or hundreds of parameters like {OPT}-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/{IKE}.},
	number = {{arXiv}:2305.12740},
	publisher = {{arXiv}},
	author = {Zheng, Ce and Li, Lei and Dong, Qingxiu and Fan, Yuxuan and Wu, Zhiyong and Xu, Jingjing and Chang, Baobao},
	urldate = {2025-01-09},
	date = {2023-05-22},
	eprinttype = {arxiv},
	eprint = {2305.12740 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1425/Zheng et al. - 2023 - Can We Edit Factual Knowledge by In-Context Learning.pdf:application/pdf;Snapshot:files/1426/2305.html:text/html},
}

@misc{mitchell_fast_2022,
	title = {Fast Model Editing at Scale},
	url = {http://arxiv.org/abs/2110.11309},
	doi = {10.48550/arXiv.2110.11309},
	abstract = {While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks using Gradient Decomposition ({MEND}), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. {MEND} learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. {MEND} can be trained on a single {GPU} in less than a day even for 10 billion+ parameter models; once trained {MEND} enables rapid application of new edits to the pre-trained model. Our experiments with T5, {GPT}, {BERT}, and {BART} models show that {MEND} is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing.},
	number = {{arXiv}:2110.11309},
	publisher = {{arXiv}},
	author = {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D.},
	urldate = {2025-01-09},
	date = {2022-06-13},
	eprinttype = {arxiv},
	eprint = {2110.11309 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1429/Mitchell et al. - 2022 - Fast Model Editing at Scale.pdf:application/pdf;Snapshot:files/1430/2110.html:text/html},
}

@misc{wang_lora-ga_2024,
	title = {{LoRA}-{GA}: Low-Rank Adaptation with Gradient Approximation},
	url = {http://arxiv.org/abs/2407.05000},
	doi = {10.48550/arXiv.2407.05000},
	shorttitle = {{LoRA}-{GA}},
	abstract = {Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs. {LoRA}, as one of the most popular Parameter-Efficient Fine-Tuning ({PEFT}) methods, offers a cost-effective alternative by fine-tuning an auxiliary low-rank model that has significantly fewer parameters. Although {LoRA} reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance. In our paper, we perform an in-depth investigation of the initialization method of {LoRA} and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. In particular, we introduce a novel initialization method, {LoRA}-{GA} (Low Rank Adaptation with Gradient Approximation), which aligns the gradients of low-rank matrix product with those of full fine-tuning at the first step. Our extensive experiments demonstrate that {LoRA}-{GA} achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla {LoRA} as well as various recent improvements) while simultaneously attaining comparable or even better performance. For example, on the subset of the {GLUE} dataset with T5-Base, {LoRA}-{GA} outperforms {LoRA} by 5.69\% on average. On larger models such as Llama 2-7B, {LoRA}-{GA} shows performance improvements of 0.34, 11.52\%, and 5.05\% on {MT}-bench, {GSM}8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla {LoRA}, validating its effectiveness in accelerating convergence and enhancing model performance. Code is available at https://github.com/Outsider565/{LoRA}-{GA}.},
	number = {{arXiv}:2407.05000},
	publisher = {{arXiv}},
	author = {Wang, Shaowen and Yu, Linxi and Li, Jian},
	urldate = {2025-01-09},
	date = {2024-07-16},
	eprinttype = {arxiv},
	eprint = {2407.05000 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1432/Wang et al. - 2024 - LoRA-GA Low-Rank Adaptation with Gradient Approximation.pdf:application/pdf;Snapshot:files/1433/2407.html:text/html},
}

@misc{biderman_lora_2024,
	title = {{LoRA} Learns Less and Forgets Less},
	url = {http://arxiv.org/abs/2405.09673},
	doi = {10.48550/arXiv.2405.09673},
	abstract = {Low-Rank Adaptation ({LoRA}) is a widely-used parameter-efficient finetuning method for large language models. {LoRA} saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of {LoRA} and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (approximately 100K prompt-response pairs) and continued pretraining (20B unstructured tokens) data regimes. Our results show that, in the standard low-rank settings, {LoRA} substantially underperforms full finetuning. Nevertheless, {LoRA} better maintains the base model's performance on tasks outside the target domain. We show that {LoRA} mitigates forgetting more than common regularization techniques such as weight decay and dropout; it also helps maintain more diverse generations. Finally, we show that full finetuning learns perturbations with a rank that is 10-100X greater than typical {LoRA} configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with {LoRA}.},
	number = {{arXiv}:2405.09673},
	publisher = {{arXiv}},
	author = {Biderman, Dan and Portes, Jacob and Ortiz, Jose Javier Gonzalez and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and Blakeney, Cody and Cunningham, John P.},
	urldate = {2025-01-09},
	date = {2024-09-20},
	eprinttype = {arxiv},
	eprint = {2405.09673 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1436/Biderman et al. - 2024 - LoRA Learns Less and Forgets Less.pdf:application/pdf;Snapshot:files/1437/2405.html:text/html},
}

@misc{hao_flora_2024,
	title = {Flora: Low-Rank Adapters Are Secretly Gradient Compressors},
	url = {http://arxiv.org/abs/2402.03293},
	doi = {10.48550/arXiv.2402.03293},
	shorttitle = {Flora},
	abstract = {Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation ({LoRA}) is proposed to reduce the optimization states by training fewer parameters. However, {LoRA} restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of {LoRA} and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.},
	number = {{arXiv}:2402.03293},
	publisher = {{arXiv}},
	author = {Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
	urldate = {2025-01-09},
	date = {2024-06-12},
	eprinttype = {arxiv},
	eprint = {2402.03293 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:files/1440/Hao et al. - 2024 - Flora Low-Rank Adapters Are Secretly Gradient Compressors.pdf:application/pdf;Snapshot:files/1441/2402.html:text/html},
}

@misc{hayou_lora_2024,
	title = {{LoRA}+: Efficient Low Rank Adaptation of Large Models},
	url = {http://arxiv.org/abs/2402.12354},
	doi = {10.48550/arXiv.2402.12354},
	shorttitle = {{LoRA}+},
	abstract = {In this paper, we show that Low Rank Adaptation ({LoRA}) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in {LoRA} are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of {LoRA} can be corrected simply by setting different learning rates for the {LoRA} adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm {LoRA}\$+\$. In our extensive experiments, {LoRA}\$+\$ improves performance (1-2 \${\textbackslash}\%\$ improvements) and finetuning speed (up to \${\textbackslash}sim\$ 2X {SpeedUp}), at the same computational cost as {LoRA}.},
	number = {{arXiv}:2402.12354},
	publisher = {{arXiv}},
	author = {Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
	urldate = {2025-01-09},
	date = {2024-07-04},
	eprinttype = {arxiv},
	eprint = {2402.12354 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:files/1444/Hayou et al. - 2024 - LoRA+ Efficient Low Rank Adaptation of Large Models.pdf:application/pdf;Snapshot:files/1445/2402.html:text/html},
}

@article{mao_survey_2025,
	title = {A Survey on {LoRA} of Large Language Models},
	volume = {19},
	issn = {2095-2228, 2095-2236},
	url = {http://arxiv.org/abs/2407.11046},
	doi = {10.1007/s11704-024-40663-9},
	abstract = {Low-Rank Adaptation{\textasciitilde}({LoRA}), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, {LoRA} has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on {LoRA}. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve {LoRA}'s performance on downstream tasks; (2) cross-task generalization methods that mix multiple {LoRA} plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of {LoRA}; (4) data privacy-preserving methods that use {LoRA} in federated learning; (5) application. Besides, this survey also discusses the future directions in this field. At last, we provide a Github page{\textasciitilde}{\textbackslash}footnote\{{\textbackslash}href\{https://github.com/{ZJU}-{LLMs}/Awesome-{LoRAs}.git\}\{https://github.com/{ZJU}-{LLMs}/Awesome-{LoRAs}.git\}\} for readers to check the updates and initiate discussions on this survey paper.},
	pages = {197605},
	number = {7},
	journaltitle = {Frontiers of Computer Science},
	shortjournal = {Front. Comput. Sci.},
	author = {Mao, Yuren and Ge, Yuhang and Fan, Yijiang and Xu, Wenyi and Mi, Yu and Hu, Zhonghao and Gao, Yunjun},
	urldate = {2025-01-09},
	date = {2025-07},
	eprinttype = {arxiv},
	eprint = {2407.11046 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1448/Mao et al. - 2025 - A Survey on LoRA of Large Language Models.pdf:application/pdf;Snapshot:files/1449/2407.html:text/html},
}

@misc{yen_lora_2024,
	title = {{LoRA} Done {RITE}: Robust Invariant Transformation Equilibration for {LoRA} Optimization},
	url = {http://arxiv.org/abs/2410.20625},
	doi = {10.48550/arXiv.2410.20625},
	shorttitle = {{LoRA} Done {RITE}},
	abstract = {Low-rank adaption ({LoRA}) is a widely used parameter-efficient finetuning method for {LLM} that reduces memory requirements. However, current {LoRA} optimizers lack transformation invariance, meaning the actual updates to the weights depends on how the two {LoRA} factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces {LoRA}-{RITE}, a novel adaptive matrix preconditioning method for {LoRA} optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various {LLM} tasks with different models including Gemma 2B, 7B, and {mT}5-{XXL}. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with {LoRA}-{RITE} during {LoRA} fine-tuning of Gemma-2B yielded 4.6{\textbackslash}\% accuracy gain on Super-Natural Instructions and 3.5{\textbackslash}\% accuracy gain across other four {LLM} benchmarks ({HellaSwag}, {ArcChallenge}, {GSM}8K, {OpenBookQA}).},
	number = {{arXiv}:2410.20625},
	publisher = {{arXiv}},
	author = {Yen, Jui-Nan and Si, Si and Meng, Zhao and Yu, Felix and Duvvuri, Sai Surya and Dhillon, Inderjit S. and Hsieh, Cho-Jui and Kumar, Sanjiv},
	urldate = {2025-01-09},
	date = {2024-10-27},
	eprinttype = {arxiv},
	eprint = {2410.20625 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1451/Yen et al. - 2024 - LoRA Done RITE Robust Invariant Transformation Equilibration for LoRA Optimization.pdf:application/pdf;Snapshot:files/1452/2410.html:text/html},
}

@misc{zhao_lora_2024,
	title = {{LoRA} Land: 310 Fine-tuned {LLMs} that Rival {GPT}-4, A Technical Report},
	url = {http://arxiv.org/abs/2405.00732},
	doi = {10.48550/arXiv.2405.00732},
	shorttitle = {{LoRA} Land},
	abstract = {Low Rank Adaptation ({LoRA}) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning ({PEFT}) of Large Language Models ({LLMs}). {LoRA} reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving {LLMs} fine-tuned with {LoRA} in real-world applications. First, we measure the quality of {LLMs} fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit {LoRA} fine-tuned models outperform base models by 34 points and {GPT}-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of {LoRAX}, an open-source Multi-{LoRA} inference server that facilitates the deployment of multiple {LoRA} fine-tuned models on a single {GPU} using shared base model weights and dynamic adapter loading. {LoRAX} powers {LoRA} Land, a web application that hosts 25 {LoRA} fine-tuned Mistral-7B {LLMs} on a single {NVIDIA} A100 {GPU} with 80GB memory. {LoRA} Land highlights the quality and cost-effectiveness of employing multiple specialized {LLMs} over a single, general-purpose {LLM}.},
	number = {{arXiv}:2405.00732},
	publisher = {{arXiv}},
	author = {Zhao, Justin and Wang, Timothy and Abid, Wael and Angus, Geoffrey and Garg, Arnav and Kinnison, Jeffery and Sherstinsky, Alex and Molino, Piero and Addair, Travis and Rishi, Devvret},
	urldate = {2025-01-09},
	date = {2024-04-29},
	eprinttype = {arxiv},
	eprint = {2405.00732 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1454/Zhao et al. - 2024 - LoRA Land 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report.pdf:application/pdf;Snapshot:files/1455/2405.html:text/html},
}

@misc{sun_improving_2024,
	title = {Improving {LoRA} in Privacy-preserving Federated Learning},
	url = {http://arxiv.org/abs/2403.12313},
	doi = {10.48550/arXiv.2403.12313},
	abstract = {Low-rank adaptation ({LoRA}) is one of the most popular task-specific parameter-efficient fine-tuning ({PEFT}) methods on pre-trained language models for its good performance and computational efficiency. {LoRA} injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning ({FL}), {LoRA} may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy ({DP}) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effective version of {LoRA}, Federated Freeze A {LoRA} ({FFA}-{LoRA}), to alleviate these challenges and further halve the communication cost of federated fine-tuning {LLMs}. The core idea of {FFA}-{LoRA} is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices. Compared to {LoRA}, {FFA}-{LoRA} is motivated by practical and theoretical benefits in privacy-preserved {FL}. Our experiments demonstrate that {FFA}-{LoRA} provides more consistent performance with better computational efficiency over vanilla {LoRA} in various {FL} tasks.},
	number = {{arXiv}:2403.12313},
	publisher = {{arXiv}},
	author = {Sun, Youbang and Li, Zitao and Li, Yaliang and Ding, Bolin},
	urldate = {2025-01-09},
	date = {2024-03-18},
	eprinttype = {arxiv},
	eprint = {2403.12313 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:files/1460/Sun et al. - 2024 - Improving LoRA in Privacy-preserving Federated Learning.pdf:application/pdf;Snapshot:files/1461/2403.html:text/html},
}

@misc{sheng_s-lora_2024,
	title = {S-{LoRA}: Serving Thousands of Concurrent {LoRA} Adapters},
	url = {http://arxiv.org/abs/2311.03285},
	doi = {10.48550/arXiv.2311.03285},
	shorttitle = {S-{LoRA}},
	abstract = {The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation ({LoRA}), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of {LoRA} adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-{LoRA}, a system designed for the scalable serving of many {LoRA} adapters. S-{LoRA} stores all adapters in the main memory and fetches the adapters used by the currently running queries to the {GPU} memory. To efficiently use the {GPU} memory and reduce fragmentation, S-{LoRA} proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and {KV} cache tensors with varying sequence lengths. Additionally, S-{LoRA} employs a novel tensor parallelism strategy and highly optimized custom {CUDA} kernels for heterogeneous batching of {LoRA} computation. Collectively, these features enable S-{LoRA} to serve thousands of {LoRA} adapters on a single {GPU} or across multiple {GPUs} with a small overhead. Compared to state-of-the-art libraries such as {HuggingFace} {PEFT} and {vLLM} (with naive support of {LoRA} serving), S-{LoRA} can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-{LoRA} enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services. The code is available at https://github.com/S-{LoRA}/S-{LoRA}},
	number = {{arXiv}:2311.03285},
	publisher = {{arXiv}},
	author = {Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman and Lee, Nicholas and Yang, Shuo and Chou, Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and Gonzalez, Joseph E. and Stoica, Ion},
	urldate = {2025-01-09},
	date = {2024-06-05},
	eprinttype = {arxiv},
	eprint = {2311.03285 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:files/1463/Sheng et al. - 2024 - S-LoRA Serving Thousands of Concurrent LoRA Adapters.pdf:application/pdf;Snapshot:files/1464/2311.html:text/html},
}

@online{nikhil_anand_where_2024,
	title = {Where are facts stored in Large Language Models?},
	url = {https://nikhilanand03.github.io/rome-editing/?utm_source=chatgpt.com},
	abstract = {hello},
	titleaddon = {Nikhil Anand},
	author = {{Nikhil Anand}},
	urldate = {2025-01-09},
	date = {2024-09-14},
	langid = {english},
	file = {Snapshot:files/1466/rome-editing.html:text/html},
}

@article{nanda_fact_2023,
	title = {Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1)},
	url = {https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall},
	shorttitle = {Fact Finding},
	abstract = {If you've come here via 3Blue1Brown, hi! If want to learn more about interpreting neural networks in general, here are some resources you might find…},
	author = {Nanda, Neel and Rajamanoharan, Senthooran and Kramár, János and Shah, Rohin},
	urldate = {2025-01-09},
	date = {2023-12-23},
	langid = {english},
	file = {Snapshot:files/1468/iGuwZTHWb6DFY3sKB.html:text/html},
}

@article{rajamanoharan_fact_2023,
	title = {Fact Finding: Simplifying the Circuit (Post 2)},
	url = {https://www.alignmentforum.org/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2},
	shorttitle = {Fact Finding},
	abstract = {This is the second post in the Google {DeepMind} mechanistic interpretability team’s investigation into how language models recall facts. This post foc…},
	author = {Rajamanoharan, Senthooran and Nanda, Neel and Kramár, János and Shah, Rohin},
	urldate = {2025-01-09},
	date = {2023-12-23},
	langid = {english},
	file = {Snapshot:files/1470/fact-finding-simplifying-the-circuit-post-2.html:text/html},
}

@article{nanda_fact_2023-1,
	title = {Fact Finding: Trying to Mechanistically Understanding Early {MLPs} (Post 3)},
	url = {https://www.alignmentforum.org/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early},
	shorttitle = {Fact Finding},
	abstract = {This is the third post in the Google {DeepMind} mechanistic interpretability team’s investigation into how language models recall facts. This post focu…},
	author = {Nanda, Neel and Rajamanoharan, Senthooran and Kramár, János and Shah, Rohin},
	urldate = {2025-01-09},
	date = {2023-12-23},
	langid = {english},
	file = {Snapshot:files/1472/CW5onXm6uZxpbpsRk.html:text/html},
}

@article{rajamanoharan_fact_2023-1,
	title = {Fact Finding: How to Think About Interpreting Memorisation (Post 4)},
	url = {https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation},
	shorttitle = {Fact Finding},
	abstract = {This is the fourth post in the Google {DeepMind} mechanistic interpretability team’s investigation into how language models recall facts. In this post,…},
	author = {Rajamanoharan, Senthooran and Nanda, Neel and Kramár, János and Shah, Rohin},
	urldate = {2025-01-09},
	date = {2023-12-23},
	langid = {english},
	file = {Snapshot:files/1474/JRcNNGJQ3xNfsxPj4.html:text/html},
}

@article{nanda_fact_2023-2,
	title = {Fact Finding: Do Early Layers Specialise in Local Processing? (Post 5)},
	url = {https://www.alignmentforum.org/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing},
	shorttitle = {Fact Finding},
	abstract = {This is the fifth post in the Google {DeepMind} mechanistic interpretability team’s investigation into how language models recall facts. This post is a…},
	author = {Nanda, Neel and Rajamanoharan, Senthooran and Kramár, János and Shah, Rohin},
	urldate = {2025-01-09},
	date = {2023-12-23},
	langid = {english},
	file = {Snapshot:files/1476/xE3Y9hhriMmL4cpsR.html:text/html},
}

@misc{barbieri_tweeteval_2020,
	title = {{TweetEval}: Unified Benchmark and Comparative Evaluation for Tweet Classification},
	url = {http://arxiv.org/abs/2010.12421},
	doi = {10.48550/arXiv.2010.12421},
	shorttitle = {{TweetEval}},
	abstract = {The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework ({TweetEval}) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.},
	number = {{arXiv}:2010.12421},
	publisher = {{arXiv}},
	author = {Barbieri, Francesco and Camacho-Collados, Jose and Neves, Leonardo and Espinosa-Anke, Luis},
	urldate = {2025-03-19},
	date = {2020-10-26},
	eprinttype = {arxiv},
	eprint = {2010.12421 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {Preprint PDF:files/1504/Barbieri et al. - 2020 - TweetEval Unified Benchmark and Comparative Evaluation for Tweet Classification.pdf:application/pdf;Snapshot:files/1505/2010.html:text/html},
}

@inproceedings{strapparava_semeval-2007_2007,
	location = {Prague, Czech Republic},
	title = {{SemEval}-2007 Task 14: Affective Text},
	url = {https://aclanthology.org/S07-1013/},
	shorttitle = {{SemEval}-2007 Task 14},
	eventtitle = {{SemEval} 2007},
	pages = {70--74},
	booktitle = {Proceedings of the Fourth International Workshop on Semantic Evaluations ({SemEval}-2007)},
	publisher = {Association for Computational Linguistics},
	author = {Strapparava, Carlo and Mihalcea, Rada},
	editor = {Agirre, Eneko and Màrquez, Lluís and Wicentowski, Richard},
	urldate = {2025-03-19},
	date = {2007-06},
	file = {Full Text PDF:files/1507/Strapparava and Mihalcea - 2007 - SemEval-2007 Task 14 Affective Text.pdf:application/pdf},
}

@online{david_watson_panas-x_1994,
	title = {The {PANAS}-X: Manual for the positive and negative affect schedule-expanded form},
	url = {https://www.researchgate.net/publication/48667272_The_PANAS-X_Manual_for_the_positive_and_negative_affect_schedule-expanded_form},
	shorttitle = {({PDF}) The {PANAS}-X},
	abstract = {{PDF} {\textbar} On Jan 1, 1999, David Watson and others published The {PANAS}-X: Manual for the positive and negative affect schedule-expanded form {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	titleaddon = {{ResearchGate}},
	author = {{David Watson} and {Lee Anna Clark}},
	urldate = {2025-03-21},
	date = {1994},
	langid = {english},
	file = {PDF:files/1510/The PANAS-X Manual for the positive and negative affect schedule-expanded form.pdf:application/pdf;Snapshot:files/1509/48667272_The_PANAS-X_Manual_for_the_positive_and_negative_affect_schedule-expanded_form.html:text/html},
}

@misc{allbert_identifying_2025,
	title = {Identifying and Manipulating Personality Traits in {LLMs} Through Activation Engineering},
	url = {http://arxiv.org/abs/2412.10427},
	doi = {10.48550/arXiv.2412.10427},
	abstract = {The field of large language models ({LLMs}) has grown rapidly in recent years, driven by the desire for better efficiency, interpretability, and safe use. Building on the novel approach of "activation engineering," this study explores personality modification in {LLMs}, drawing inspiration from research like Refusal in {LLMs} Is Mediated by a Single Direction ({arXiv}:2406.11717) and Steering Llama 2 via Contrastive Activation Addition ({arXiv}:2312.06681). We leverage activation engineering to develop a method for identifying and adjusting activation directions related to personality traits, which may allow for dynamic {LLM} personality fine-tuning. This work aims to further our understanding of {LLM} interpretability while examining the ethical implications of such developments.},
	number = {{arXiv}:2412.10427},
	publisher = {{arXiv}},
	author = {Allbert, Rumi A. and Wiles, James K. and Grankovsky, Vlad},
	urldate = {2025-03-29},
	date = {2025-01-10},
	eprinttype = {arxiv},
	eprint = {2412.10427 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1521/Allbert et al. - 2025 - Identifying and Manipulating Personality Traits in LLMs Through Activation Engineering.pdf:application/pdf},
}

@article{zador_critique_2019,
	title = {A critique of pure learning and what artificial neural networks can learn from animal brains},
	volume = {10},
	rights = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-11786-6},
	doi = {10.1038/s41467-019-11786-6},
	abstract = {Artificial neural networks ({ANNs}) have undergone a revolution, catalyzed by better supervised learning algorithms. However, in stark contrast to young animals (including humans), training such networks requires enormous numbers of labeled examples, leading to the belief that animals must rely instead mainly on unsupervised learning. Here we argue that most animal behavior is not the result of clever learning algorithms—supervised or unsupervised—but is encoded in the genome. Specifically, animals are born with highly structured brain connectivity, which enables them to learn very rapidly. Because the wiring diagram is far too complex to be specified explicitly in the genome, it must be compressed through a “genomic bottleneck”. The genomic bottleneck suggests a path toward {ANNs} capable of rapid learning.},
	pages = {3770},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Zador, Anthony M.},
	urldate = {2025-05-06},
	date = {2019-08-21},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Network models},
	file = {Full Text PDF:files/1524/Zador - 2019 - A critique of pure learning and what artificial neural networks can learn from animal brains.pdf:application/pdf},
}

@article{hochschild_emotion_1979,
	title = {Emotion Work, Feeling Rules, and Social Structure},
	volume = {85},
	issn = {0002-9602},
	url = {https://www.jstor.org/stable/2778583},
	abstract = {This essay proposes an emotion-management perspective as a lens through which to inspect the self, interaction, and structure. Emotion, it is argued, can be and ofter is subject to acts of management. The individual often works on inducing or inhibiting feelings so as to render them "appropriate" to a situation. The emotion-management perspective draws on an interactive account of emotion. It differs from the dramaturgical perspective on the one hand and the psychoanalytic perspective on the other. It allows us to inspect at closer range than either of those perspectives the relation among emotive experience, emotion management, feeling rules, and ideology. Feeling rules are seen as the side of ideology that deals with emotion and feeling. Emotion management is the type of work it takes to cope with feeling rules. Meaning-making jobs, more common in the middle class, put more premium on the individual's capacity to do emotion work. A reexamination of class differences in child rearing suggest that middle-class families prepare their children for emotion management more and working-class families prepare them less. In this way each prepares its children to psychologically reproduce the class structure.},
	pages = {551--575},
	number = {3},
	journaltitle = {American Journal of Sociology},
	author = {Hochschild, Arlie Russell},
	urldate = {2025-05-21},
	date = {1979},
	note = {Publisher: The University of Chicago Press},
	file = {JSTOR Full Text PDF:files/1526/Hochschild - 1979 - Emotion Work, Feeling Rules, and Social Structure.pdf:application/pdf},
}

@article{parkinson_current_2015,
	title = {Current Emotion Research in Social Psychology: Thinking About Emotions and Other People},
	volume = {7},
	issn = {1754-0739},
	url = {https://doi.org/10.1177/1754073915590624},
	doi = {10.1177/1754073915590624},
	shorttitle = {Current Emotion Research in Social Psychology},
	abstract = {This article discusses contemporary social psychological approaches to (a) the social relations and appraisals associated with specific emotions; (b) other people’s impact on appraisal processes; (c) effects of emotion on other people; and (d) interpersonal emotion regulation. We argue that single-minded cognitive perspectives restrict our understanding of interpersonal and group-related emotional processes, and that new methodologies addressing real-time interpersonal and group processes present promising opportunities for future progress.},
	pages = {371--380},
	number = {4},
	journaltitle = {Emotion Review},
	author = {Parkinson, Brian and Manstead, Antony S. R.},
	urldate = {2025-05-21},
	date = {2015-10-01},
	note = {Publisher: {SAGE} Publications},
	file = {SAGE PDF Full Text:files/1530/Parkinson and Manstead - 2015 - Current Emotion Research in Social Psychology Thinking About Emotions and Other People.pdf:application/pdf},
}

@article{paul_towards_2020,
	title = {Towards a comparative science of emotion: Affect and consciousness in humans and animals},
	volume = {108},
	issn = {0149-7634},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763419303677},
	doi = {10.1016/j.neubiorev.2019.11.014},
	shorttitle = {Towards a comparative science of emotion},
	abstract = {The componential view of human emotion recognises that affective states comprise conscious, behavioural, physiological, neural and cognitive elements. Although many animals display bodily and behavioural changes consistent with the occurrence of affective states similar to those seen in humans, the question of whether and in which species these are accompanied by conscious experiences remains controversial. Finding scientifically valid methods for investigating markers for the subjective component of affect in both humans and animals is central to developing a comparative understanding of the processes and mechanisms of affect and its evolution and distribution across taxonomic groups, to our understanding of animal welfare, and to the development of animal models of affective disorders. Here, contemporary evidence indicating potential markers of conscious processing in animals is reviewed, with a view to extending this search to include markers of conscious affective processing. We do this by combining animal-focused approaches with investigations of the components of conscious and non-conscious emotional processing in humans, and neuropsychological research into the structure and functions of conscious emotions.},
	pages = {749--770},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	shortjournal = {Neuroscience \& Biobehavioral Reviews},
	author = {Paul, Elizabeth S. and Sher, Shlomi and Tamietto, Marco and Winkielman, Piotr and Mendl, Michael T.},
	urldate = {2025-05-21},
	date = {2020-01-01},
	keywords = {Affect, Animals, Componential, Consciousness, Interoception, Neural correlates, Subjective emotion, Unconscious emotion},
	file = {Full Text:files/1536/Paul et al. - 2020 - Towards a comparative science of emotion Affect and consciousness in humans and animals.pdf:application/pdf;ScienceDirect Snapshot:files/1535/S0149763419303677.html:text/html},
}

@article{mun_science_2022,
	title = {The Science of Emotion: Mind, Body, and Culture},
	volume = {7},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2409-9287},
	url = {https://www.mdpi.com/2409-9287/7/6/144},
	doi = {10.3390/philosophies7060144},
	shorttitle = {The Science of Emotion},
	abstract = {In this paper, I give readers an idea of what some scholars are interested in, what I found interesting, and what may be of future interest in the philosophy of emotion. I begin with a brief overview of the general topics of interests in the philosophy of emotion. I then discuss what I believe to be some of the most interesting topics in the contemporary discourse, including questions about how philosophy can inform the science of emotion, responses to aspects of the mind–body problem, and concerns about perception, cognition, and emotion, along with questions about the place of 4E approaches and meta-semantic pluraliste approaches in the embodied cognitive tradition. I also discuss the natural kind–social construction debate in the philosophy of emotion, the emerging field of cultural evolution, the import of a dual-inheritance theory in this emerging field, and I propose a possible way to integrate the frameworks of dual-inheritance theory and meta-semantic pluralisme to demonstrate at least one way in which the philosophy of emotion can contribute to the emerging field of cultural evolution. I conclude with a brief summary of this paper and note at least one significant implication of my proposal for the natural kind–social construction debate in the philosophy of emotion.},
	pages = {144},
	number = {6},
	journaltitle = {Philosophies},
	author = {Mun, Cecilea},
	urldate = {2025-05-21},
	date = {2022-12},
	langid = {english},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cognition, cultural evolution, dual-inheritance theory, embodied cognition, emotion, evolutionary norm psychology, meta-semantic pluralism, mind, mind–body problem, natural kind, perception, philosophy of emotion, science of emotion, social construction},
	file = {Full Text PDF:files/1538/Mun - 2022 - The Science of Emotion Mind, Body, and Culture.pdf:application/pdf},
}

@article{barsade_ripple_2002,
	title = {The Ripple Effect: Emotional Contagion and its Influence on Group Behavior},
	volume = {47},
	issn = {0001-8392},
	url = {https://journals.sagepub.com/action/showAbstract},
	doi = {10.2307/3094912},
	shorttitle = {The Ripple Effect},
	abstract = {Group emotional contagion, the transfer of moods among people in a group, and its influence on work group dynamics was examined in a laboratory study of managerial decision making using multiple, convergent measures of mood, individual attitudes, behavior, and group-level dynamics. Using a 2 times 2 experimental design, with a trained confederate enacting mood conditions, the predicted effect of emotional contagion was found among group members, using both outside coders' ratings of participants' mood and participants' self-reported mood. No hypothesized differences in contagion effects due to the degree of pleasantness of the mood expressed and the energy level with which it was conveyed were found. There was a significant influence of emotional contagion on individual-level attitudes and group processes. As predicted, the positive emotional contagion group members experienced improved cooperation, decreased conflict, and increased perceived task performance. Theoretical implications and practical ramifications of emotional contagion in groups and organizations are discussed.},
	pages = {644--675},
	number = {4},
	journaltitle = {Administrative Science Quarterly},
	author = {Barsade, Sigal G.},
	urldate = {2025-05-21},
	date = {2002-12-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:files/1540/Barsade - 2002 - The Ripple Effect Emotional Contagion and its Influence on Group Behavior.pdf:application/pdf},
}

@article{nummenmaa_bodily_2014,
	title = {Bodily maps of emotions},
	volume = {111},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1321664111},
	doi = {10.1073/pnas.1321664111},
	abstract = {Emotions are often felt in the body, and somatosensory feedback has been proposed to trigger conscious emotional experiences. Here we reveal maps of bodily sensations associated with different emotions using a unique topographical self-report method. In five experiments, participants (n = 701) were shown two silhouettes of bodies alongside emotional words, stories, movies, or facial expressions. They were asked to color the bodily regions whose activity they felt increasing or decreasing while viewing each stimulus. Different emotions were consistently associated with statistically separable bodily sensation maps across experiments. These maps were concordant across West European and East Asian samples. Statistical classifiers distinguished emotion-specific activation maps accurately, confirming independence of topographies across emotions. We propose that emotions are represented in the somatosensory system as culturally universal categorical somatotopic maps. Perception of these emotion-triggered bodily changes may play a key role in generating consciously felt emotions.},
	pages = {646--651},
	number = {2},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Nummenmaa, Lauri and Glerean, Enrico and Hari, Riitta and Hietanen, Jari K.},
	urldate = {2025-05-21},
	date = {2014-01-14},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:files/1548/Nummenmaa et al. - 2014 - Bodily maps of emotions.pdf:application/pdf},
}

@article{van_kleef_how_2009,
	title = {How Emotions Regulate Social Life: The Emotions as Social Information ({EASI}) Model},
	volume = {18},
	issn = {0963-7214},
	url = {https://doi.org/10.1111/j.1467-8721.2009.01633.x},
	doi = {10.1111/j.1467-8721.2009.01633.x},
	shorttitle = {How Emotions Regulate Social Life},
	abstract = {The idea that emotions regulate social interaction is increasingly popular. But exactly how do emotions do this? To address this question, I draw on research on the interpersonal effects of emotions on behavior in personal relationships, parent–child interactions, conflict, negotiation, and leadership, and propose a new framework that can account for existing findings and guide future research: the emotions as social information ({EASI}) model. I demonstrate that emotional expressions affect observers' behavior by triggering inferential processes and/or affective reactions in them. The predictive strength of these two processes—which may inspire different behaviors—depends on the observer's information processing and on social-relational factors. Examples of moderators that determine the relative predictive strength of inferences and affective reactions include power, need for cognitive closure, time pressure, display rules, and the appropriateness and target of the emotional expression, which are all discussed.},
	pages = {184--188},
	number = {3},
	journaltitle = {Current Directions in Psychological Science},
	shortjournal = {Curr Dir Psychol Sci},
	author = {Van Kleef, Gerben A.},
	urldate = {2025-05-21},
	date = {2009-06-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:files/1552/Van Kleef - 2009 - How Emotions Regulate Social Life The Emotions as Social Information (EASI) Model.pdf:application/pdf},
}

@article{kosinski_private_2013,
	title = {Private traits and attributes are predictable from digital records of human behavior},
	volume = {110},
	url = {https://www.pnas.org/doi/10.1073/pnas.1218772110},
	doi = {10.1073/pnas.1218772110},
	abstract = {We show that easily accessible digital records of behavior, Facebook Likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. The analysis presented is based on a dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests. The proposed model uses dimensionality reduction for preprocessing the Likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from Likes. The model correctly discriminates between homosexual and heterosexual men in 88\% of cases, African Americans and Caucasian Americans in 95\% of cases, and between Democrat and Republican in 85\% of cases. For the personality trait “Openness,” prediction accuracy is close to the test–retest accuracy of a standard personality test. We give examples of associations between attributes and Likes and discuss implications for online personalization and privacy.},
	pages = {5802--5805},
	number = {15},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Kosinski, Michal and Stillwell, David and Graepel, Thore},
	urldate = {2025-05-21},
	date = {2013-04-09},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:files/1558/Kosinski et al. - 2013 - Private traits and attributes are predictable from digital records of human behavior.pdf:application/pdf},
}

@article{matz_psychological_2017,
	title = {Psychological targeting as an effective approach to digital mass persuasion},
	volume = {114},
	url = {https://www.pnas.org/doi/10.1073/pnas.1710966114},
	doi = {10.1073/pnas.1710966114},
	abstract = {People are exposed to persuasive communication across many different contexts: Governments, companies, and political parties use persuasive appeals to encourage people to eat healthier, purchase a particular product, or vote for a specific candidate. Laboratory studies show that such persuasive appeals are more effective in influencing behavior when they are tailored to individuals’ unique psychological characteristics. However, the investigation of large-scale psychological persuasion in the real world has been hindered by the questionnaire-based nature of psychological assessment. Recent research, however, shows that people’s psychological characteristics can be accurately predicted from their digital footprints, such as their Facebook Likes or Tweets. Capitalizing on this form of psychological assessment from digital footprints, we test the effects of psychological persuasion on people’s actual behavior in an ecologically valid setting. In three field experiments that reached over 3.5 million individuals with psychologically tailored advertising, we find that matching the content of persuasive appeals to individuals’ psychological characteristics significantly altered their behavior as measured by clicks and purchases. Persuasive appeals that were matched to people’s extraversion or openness-to-experience level resulted in up to 40\% more clicks and up to 50\% more purchases than their mismatching or unpersonalized counterparts. Our findings suggest that the application of psychological targeting makes it possible to influence the behavior of large groups of people by tailoring persuasive appeals to the psychological needs of the target audiences. We discuss both the potential benefits of this method for helping individuals make better decisions and the potential pitfalls related to manipulation and privacy.},
	pages = {12714--12719},
	number = {48},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Matz, S. C. and Kosinski, M. and Nave, G. and Stillwell, D. J.},
	urldate = {2025-05-21},
	date = {2017-11-28},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:files/1560/Matz et al. - 2017 - Psychological targeting as an effective approach to digital mass persuasion.pdf:application/pdf},
}

@misc{wang_emotional_2023,
	title = {Emotional Intelligence of Large Language Models},
	url = {http://arxiv.org/abs/2307.09042},
	doi = {10.48550/arXiv.2307.09042},
	abstract = {Large Language Models ({LLMs}) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed {LLMs}' Emotional Intelligence ({EI}), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding ({EU}), a core component of {EI}, suitable for both humans and {LLMs}. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream {LLMs}. Most achieved above-average {EQ} scores, with {GPT}-4 exceeding 89\% of human participants with an {EQ} of 117. Interestingly, a multivariate pattern analysis revealed that some {LLMs} apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on {LLMs}' {EQ}. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of {LLMs}, which may shed light on the future development of {LLMs} aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/},
	number = {{arXiv}:2307.09042},
	publisher = {{arXiv}},
	author = {Wang, Xuena and Li, Xueting and Yin, Zi and Wu, Yue and Jia, Liu},
	urldate = {2025-05-21},
	date = {2023-07-28},
	eprinttype = {arxiv},
	eprint = {2307.09042 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1566/Wang et al. - 2023 - Emotional Intelligence of Large Language Models.pdf:application/pdf;Snapshot:files/1565/2307.html:text/html},
}

@misc{zhao_both_2024,
	title = {Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence},
	url = {http://arxiv.org/abs/2402.10073},
	doi = {10.48550/arXiv.2402.10073},
	shorttitle = {Both Matter},
	abstract = {Emotional Intelligence ({EI}), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model ({LLM}) based conversational general {AI} assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on {EI}-related classification or regression tasks. However, this leads to the incomplete enhancement of {EI} and catastrophic forgetting of the general intelligence ({GI}). To this end, we first introduce {\textbackslash}textsc\{{EiBench}\}, a large-scale collection of {EI}-related tasks in the text-to-text formation with task instructions that covers all three aspects of {EI}, which lays a solid foundation for the comprehensive {EI} enhancement of {LLMs}. Then a novel {\textbackslash}underline\{{\textbackslash}textbf\{Mo\}\}dular {\textbackslash}underline\{{\textbackslash}textbf\{E\}\}motional {\textbackslash}underline\{{\textbackslash}textbf\{I\}\}ntelligence enhancement method ({\textbackslash}textbf\{{MoEI}\}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the {EI} of {LLMs} without compromise their {GI}. Extensive experiments on two representative {LLM}-based assistants, Flan-T5 and {LLaMA}-2-Chat, demonstrate the effectiveness of {MoEI} to improving {EI} while maintain {GI}.},
	number = {{arXiv}:2402.10073},
	publisher = {{arXiv}},
	author = {Zhao, Weixiang and Li, Zhuojun and Wang, Shilong and Wang, Yang and Hu, Yulin and Zhao, Yanyan and Wei, Chen and Qin, Bing},
	urldate = {2025-05-21},
	date = {2024-06-12},
	eprinttype = {arxiv},
	eprint = {2402.10073 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1570/Zhao et al. - 2024 - Both Matter Enhancing the Emotional Intelligence of Large Language Models without Compromising the.pdf:application/pdf;Snapshot:files/1569/2402.html:text/html},
}

@misc{sabour_emobench_2024,
	title = {{EmoBench}: Evaluating the Emotional Intelligence of Large Language Models},
	url = {http://arxiv.org/abs/2402.12071},
	doi = {10.48550/arXiv.2402.12071},
	shorttitle = {{EmoBench}},
	abstract = {Recent advances in Large Language Models ({LLMs}) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence ({EI}) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential {EI} capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose {EmoBench}, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine {EI}, including Emotional Understanding and Emotional Application. {EmoBench} includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the {EI} of existing {LLMs} and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/{EmoBench}.},
	number = {{arXiv}:2402.12071},
	publisher = {{arXiv}},
	author = {Sabour, Sahand and Liu, Siyang and Zhang, Zheyuan and Liu, June M. and Zhou, Jinfeng and Sunaryo, Alvionna S. and Li, Juanzi and Lee, Tatia M. C. and Mihalcea, Rada and Huang, Minlie},
	urldate = {2025-05-21},
	date = {2024-07-17},
	eprinttype = {arxiv},
	eprint = {2402.12071 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1573/Sabour et al. - 2024 - EmoBench Evaluating the Emotional Intelligence of Large Language Models.pdf:application/pdf;Snapshot:files/1574/2402.html:text/html},
}

@inproceedings{liu_emollms_2024,
	title = {{EmoLLMs}: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis},
	url = {http://arxiv.org/abs/2401.08508},
	doi = {10.1145/3637528.3671552},
	shorttitle = {{EmoLLMs}},
	abstract = {Sentiment analysis and emotion detection are important research topics in natural language processing ({NLP}) and benefit many downstream tasks. With the widespread application of {LLMs}, researchers have started exploring the application of {LLMs} based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose {EmoLLMs}, the first series of open-sourced instruction-following {LLMs} for comprehensive affective analysis based on fine-tuning various {LLMs} with instruction data, the first multi-task affective analysis instruction dataset ({AAID}) with 234K data samples based on various classification and regression tasks to support {LLM} instruction tuning, and a comprehensive affective evaluation benchmark ({AEB}) with 14 tasks from various sources and domains to test the generalization ability of {LLMs}. We propose a series of {EmoLLMs} by fine-tuning {LLMs} with {AAID} to solve various affective instruction tasks. We compare our model with a variety of {LLMs} on {AEB}, where our models outperform all other open-sourced {LLMs}, and surpass {ChatGPT} and {GPT}-4 in most tasks, which shows that the series of {EmoLLMs} achieve the {ChatGPT}-level and {GPT}-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools.},
	pages = {5487--5496},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	author = {Liu, Zhiwei and Yang, Kailai and Zhang, Tianlin and Xie, Qianqian and Ananiadou, Sophia},
	urldate = {2025-05-21},
	date = {2024-08-25},
	eprinttype = {arxiv},
	eprint = {2401.08508 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:files/1579/Liu et al. - 2024 - EmoLLMs A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affectiv.pdf:application/pdf;Snapshot:files/1578/2401.html:text/html},
}

@misc{mozikov_good_2024,
	title = {The Good, the Bad, and the Hulk-like {GPT}: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games},
	url = {http://arxiv.org/abs/2406.03299},
	doi = {10.48550/arXiv.2406.03299},
	shorttitle = {The Good, the Bad, and the Hulk-like {GPT}},
	abstract = {Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models ({LLMs}) have provided researchers with a new promising tool for the simulation of human behavior. However, existing {LLM}-based simulations operate under the unproven hypothesis that {LLM} agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions. In this paper, we introduce a novel methodology and the framework to study both, the decision-making of {LLMs} and their alignment with human behavior under emotional states. Experiments with {GPT}-3.5 and {GPT}-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of {LLMs}, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of {GPT}-3.5 and human participants, particularly evident in bargaining games, {GPT}-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the "superhuman" alignment of {GPT}-4, resembling human emotional responses.},
	number = {{arXiv}:2406.03299},
	publisher = {{arXiv}},
	author = {Mozikov, Mikhail and Severin, Nikita and Bodishtianu, Valeria and Glushanina, Maria and Baklashkin, Mikhail and Savchenko, Andrey V. and Makarov, Ilya},
	urldate = {2025-05-21},
	date = {2024-06-05},
	eprinttype = {arxiv},
	eprint = {2406.03299 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1582/Mozikov et al. - 2024 - The Good, the Bad, and the Hulk-like GPT Analyzing Emotional Decisions of Large Language Models in.pdf:application/pdf;Snapshot:files/1581/2406.html:text/html},
}

@misc{chang_modeling_2024,
	title = {Modeling Emotions and Ethics with Large Language Models},
	url = {http://arxiv.org/abs/2404.13071},
	doi = {10.48550/arXiv.2404.13071},
	abstract = {This paper explores the integration of human-like emotions and ethical considerations into Large Language Models ({LLMs}). We first model eight fundamental human emotions, presented as opposing pairs, and employ collaborative {LLMs} to reinterpret and express these emotions across a spectrum of intensity. Our focus extends to embedding a latent ethical dimension within {LLMs}, guided by a novel self-supervised learning algorithm with human feedback ({SSHF}). This approach enables {LLMs} to perform self-evaluations and adjustments concerning ethical guidelines, enhancing their capability to generate content that is not only emotionally resonant but also ethically aligned. The methodologies and case studies presented herein illustrate the potential of {LLMs} to transcend mere text and image generation, venturing into the realms of empathetic interaction and principled decision-making, thereby setting a new precedent in the development of emotionally aware and ethically conscious {AI} systems.},
	number = {{arXiv}:2404.13071},
	publisher = {{arXiv}},
	author = {Chang, Edward Y.},
	urldate = {2025-05-21},
	date = {2024-06-25},
	eprinttype = {arxiv},
	eprint = {2404.13071 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1586/Chang - 2024 - Modeling Emotions and Ethics with Large Language Models.pdf:application/pdf;Snapshot:files/1585/2404.html:text/html},
}

@misc{paech_eq-bench_2024,
	title = {{EQ}-Bench: An Emotional Intelligence Benchmark for Large Language Models},
	url = {http://arxiv.org/abs/2312.06281},
	doi = {10.48550/arXiv.2312.06281},
	shorttitle = {{EQ}-Bench},
	abstract = {We introduce {EQ}-Bench, a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models ({LLMs}). We assess the ability of {LLMs} to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue. The benchmark is able to discriminate effectively between a wide range of models. We find that {EQ}-Bench correlates strongly with comprehensive multi-domain benchmarks like {MMLU} (Hendrycks et al., 2020) (r=0.97), indicating that we may be capturing similar aspects of broad intelligence. Our benchmark produces highly repeatable results using a set of 60 English-language questions. We also provide open-source code for an automated benchmarking pipeline at https://github.com/{EQ}-bench/{EQ}-Bench and a leaderboard at https://eqbench.com},
	number = {{arXiv}:2312.06281},
	publisher = {{arXiv}},
	author = {Paech, Samuel J.},
	urldate = {2025-05-21},
	date = {2024-01-03},
	eprinttype = {arxiv},
	eprint = {2312.06281 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/1589/Paech - 2024 - EQ-Bench An Emotional Intelligence Benchmark for Large Language Models.pdf:application/pdf;Snapshot:files/1588/2312.html:text/html},
}

@misc{zhang_sentient_2025,
	title = {Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models},
	url = {http://arxiv.org/abs/2505.02847},
	doi = {10.48550/arXiv.2505.02847},
	shorttitle = {Sentient Agent as a Judge},
	abstract = {Assessing how well a large language model ({LLM}) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge ({SAGE}), an automated evaluation framework that measures an {LLM}'s higher-order social cognition. {SAGE} instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory ({BLRI}) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems ({GPT}-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). {SAGE} thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.},
	number = {{arXiv}:2505.02847},
	publisher = {{arXiv}},
	author = {Zhang, Bang and Ma, Ruotian and Jiang, Qingxuan and Wang, Peisong and Chen, Jiaqi and Xie, Zheng and Chen, Xingyu and Wang, Yue and Ye, Fanghua and Li, Jian and Yang, Yifan and Tu, Zhaopeng and Li, Xiaolong},
	urldate = {2025-05-21},
	date = {2025-05-09},
	eprinttype = {arxiv},
	eprint = {2505.02847 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Preprint PDF:files/1593/Zhang et al. - 2025 - Sentient Agent as a Judge Evaluating Higher-Order Social Cognition in Large Language Models.pdf:application/pdf;Snapshot:files/1592/2505.html:text/html},
}

@article{johnson_measuring_2014,
	title = {Measuring thirty facets of the Five Factor Model with a 120-item public domain inventory: Development of the {IPIP}-{NEO}-120},
	volume = {51},
	issn = {00926566},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092656614000506},
	doi = {10.1016/j.jrp.2014.05.003},
	shorttitle = {Measuring thirty facets of the Five Factor Model with a 120-item public domain inventory},
	pages = {78--89},
	journaltitle = {Journal of Research in Personality},
	shortjournal = {Journal of Research in Personality},
	author = {Johnson, John A.},
	urldate = {2025-05-21},
	date = {2014-08},
	langid = {english},
	file = {PDF:files/1598/Johnson - 2014 - Measuring thirty facets of the Five Factor Model with a 120-item public domain inventory Developmen.pdf:application/pdf},
}

@article{maples_test_2014,
	title = {A test of the International Personality Item Pool representation of the Revised {NEO} Personality Inventory and development of a 120-item {IPIP}-based measure of the five-factor model},
	volume = {26},
	issn = {1939-134X},
	doi = {10.1037/pas0000004},
	abstract = {There has been a substantial increase in the use of personality assessment measures constructed using items from the International Personality Item Pool ({IPIP}) such as the 300-item {IPIP}-{NEO} (Goldberg, 1999), a representation of the Revised {NEO} Personality Inventory ({NEO} {PI}-R; Costa \& {McCrae}, 1992). The {IPIP}-{NEO} is free to use and can be modified to accommodate its users' needs. Despite the substantial interest in this measure, there is still a dearth of data demonstrating its convergence with the {NEO} {PI}-R. The present study represents an investigation of the reliability and validity of scores on the {IPIP}-{NEO}. Additionally, we used item response theory ({IRT}) methodology to create a 120-item version of the {IPIP}-{NEO}. Using an undergraduate sample (n = 359), we examined the reliability, as well as the convergent and criterion validity, of scores from the 300-item {IPIP}-{NEO}, a previously constructed 120-item version of the {IPIP}-{NEO} (Johnson, 2011), and the newly created {IRT}-based {IPIP}-120 in comparison to the {NEO} {PI}-R across a range of outcomes. Scores from all 3 {IPIP} measures demonstrated strong reliability and convergence with the {NEO} {PI}-R and a high degree of similarity with regard to their correlational profiles across the criterion variables ({rICC} = .983, .972, and .976, respectively). The replicability of these findings was then tested in a community sample (n = 757), and the results closely mirrored the findings from Sample 1. These results provide support for the use of the {IPIP}-{NEO} and both 120-item {IPIP}-{NEO} measures as assessment tools for measurement of the five-factor model.},
	pages = {1070--1084},
	number = {4},
	journaltitle = {Psychological Assessment},
	shortjournal = {Psychol Assess},
	author = {Maples, Jessica L. and Guan, Li and Carter, Nathan T. and Miller, Joshua D.},
	date = {2014-12},
	pmid = {24932643},
	keywords = {Female, Humans, Male, Psychometrics, Reproducibility of Results, Adult, Adolescent, Aged, Aged, 80 and over, Factor Analysis, Statistical, Middle Aged, Personality Assessment, Personality Disorders, Personality Inventory, Young Adult},
	file = {PDF:files/1603/Maples et al. - 2014 - A test of the International Personality Item Pool representation of the Revised NEO Personality Inve.pdf:application/pdf},
}

@article{kajonius_assessing_2019,
	title = {Assessing the Structure of the Five Factor Model of Personality ({IPIP}-{NEO}-120) in the Public Domain},
	volume = {15},
	issn = {1841-0413},
	doi = {10.5964/ejop.v15i2.1671},
	abstract = {Assessment of individual differences in personality traits is arguably one of the hallmarks of psychological research. Testing the structural validity of trait measurements is paramount in this endeavor. In the current study, we investigated 30 facet traits in one of the accessible and comprehensive public-domain Five Factor Model ({FFM}) personality inventories, {IPIP}-{NEO}-120 (Johnson, 2014), using one of the largest {US} samples to date (N = 320,128). We present structural loadings for all trait facets organized into respective {FFM}-trait domain (Neuroticism, Extraversion, Openness, Agreeableness, and Conscientiousness). Both hierarchical second-order and bi-factor models showed tolerable model fit indices, using confirmatory factor analysis in a structural equation modeling ({SEM}) framework. Some facet traits were substantially more representative than others for their respective trait domain, which facilitate further discussions on {FFM}-construct content. We conclude that {IPIP}-{NEO} is sufficiently structurally robust for future use, for the benefit of research and practice in personality assessment.},
	pages = {260--275},
	number = {2},
	journaltitle = {Europe's Journal of Psychology},
	shortjournal = {Eur J Psychol},
	author = {Kajonius, Petri J. and Johnson, John A.},
	date = {2019-06},
	pmid = {33574954},
	pmcid = {PMC7871748},
	keywords = {{IPIP}, Five Factor Model, personality, structure},
	file = {Full Text PDF:files/1606/Kajonius and Johnson - 2019 - Assessing the Structure of the Five Factor Model of Personality (IPIP-NEO-120) in the Public Domain.pdf:application/pdf},
}

@article{lerner_emotion_2015,
	title = {Emotion and Decision Making},
	volume = {66},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-psych-010213-115043},
	doi = {10.1146/annurev-psych-010213-115043},
	abstract = {A revolution in the science of emotion has emerged in recent decades, with the potential to create a paradigm shift in decision theories. The research reveals that emotions constitute potent, pervasive, predictable, sometimes harmful and sometimes beneficial drivers of decision making. Across different domains, important regularities appear in the mechanisms through which emotions influence judgments and choices. We organize and analyze what has been learned from the past 35 years of work on emotion and decision making. In so doing, we propose the emotion-imbued choice model, which accounts for inputs from traditional rational choice theory and from newer emotion research, synthesizing scientific models.},
	pages = {799--823},
	issue = {Volume 66, 2015},
	journaltitle = {Annual Review of Psychology},
	author = {Lerner, Jennifer S. and Li, Ye and Valdesolo, Piercarlo and Kassam, Karim S.},
	urldate = {2025-05-21},
	date = {2015-01-03},
	langid = {english},
	note = {Publisher: Annual Reviews},
	file = {annurev-psych-010213-115043:files/1610/annurev-psych-010213-115043.pdf:application/pdf;Snapshot:files/1609/annurev-psych-010213-115043.html:text/html},
}

@misc{bengio_superintelligent_2025,
	title = {Superintelligent Agents Pose Catastrophic Risks: Can Scientist {AI} Offer a Safer Path?},
	url = {http://arxiv.org/abs/2502.15657},
	doi = {10.48550/arXiv.2502.15657},
	shorttitle = {Superintelligent Agents Pose Catastrophic Risks},
	abstract = {The leading {AI} companies are increasingly focused on building generalist {AI} agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked {AI} agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current {AI} training methods. Indeed, various scenarios and experiments have demonstrated the possibility of {AI} agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic {AI} system that is trustworthy and safe by design, which we call Scientist {AI}. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist {AI} could be used to assist human researchers in accelerating scientific progress, including in {AI} safety. In particular, our system can be employed as a guardrail against {AI} agents that might be created despite the risks involved. Ultimately, focusing on non-agentic {AI} may enable the benefits of {AI} innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path.},
	number = {{arXiv}:2502.15657},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Cohen, Michael and Fornasiere, Damiano and Ghosn, Joumana and Greiner, Pietro and {MacDermott}, Matt and Mindermann, Sören and Oberman, Adam and Richardson, Jesse and Richardson, Oliver and Rondeau, Marc-Antoine and St-Charles, Pierre-Luc and Williams-King, David},
	urldate = {2025-05-22},
	date = {2025-02-24},
	eprinttype = {arxiv},
	eprint = {2502.15657 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/1614/Bengio et al. - 2025 - Superintelligent Agents Pose Catastrophic Risks Can Scientist AI Offer a Safer Path.pdf:application/pdf;Snapshot:files/1613/2502.html:text/html},
}
